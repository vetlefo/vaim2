// File: .gitignore
----------------------------------------
# Dependency directories
node_modules/
dist/
build/
.coverage/

# Environment files
.env
.env.*
!.env.example

# Logs
logs/
*.log
npm-debug.log*

# OS generated files
Thumbs.db
.DS_Store

# IDE
.vscode/
.idea/

# Testing
coverage/
junit.xml

# Package manager specific
package-lock.json
yarn.lock


// File: docker-compose.yml
----------------------------------------
version: '3.8'

services:
  auth-service:
    build: ./services/auth-service
    ports:
      - "3001:3000"
    depends_on:
      - auth-postgres
      - auth-neo4j
    environment:
      NODE_ENV: development
      DB_HOST: auth-postgres
      DB_PORT: 5432
      DB_USER: ${POSTGRES_USER:-admin}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-admin}
      DB_NAME: ${POSTGRES_DB:-auth}
      NEO4J_URI: bolt://auth-neo4j:7687
      NEO4J_USER: ${NEO4J_USER:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-password123}
    networks:
      - auth-network

  graph-analytics-service:
    build: ./services/graph-analytics-service
    ports:
      - "3002:3002"
    depends_on:
      - auth-neo4j
      - kafka
      - redis
    environment:
      NODE_ENV: development
      PORT: 3002
      NEO4J_URI: bolt://auth-neo4j:7687
      NEO4J_USERNAME: ${NEO4J_USER:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-password123}
      NEO4J_DATABASE: neo4j
      KAFKA_BROKERS: kafka:9092
      KAFKA_CLIENT_ID: graph-analytics-service
      KAFKA_GROUP_ID: graph-analytics-group
      REDIS_HOST: redis
      REDIS_PORT: 6379
      JWT_SECRET: ${JWT_SECRET:-your-jwt-secret}
    networks:
      - auth-network
      - analytics-network

  auth-postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-admin}
      POSTGRES_DB: ${POSTGRES_DB:-auth}
    ports:
      - "5433:5432"  # Changed from 5432 to avoid conflicts
    volumes:
      - auth-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-admin} -d ${POSTGRES_DB:-auth}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - auth-network

  auth-neo4j:
    image: neo4j:5-enterprise
    environment:
      NEO4J_AUTH: ${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-password123}
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      NEO4J_dbms_security_procedures_unrestricted: "gds.*"
      NEO4J_dbms_security_procedures_whitelist: "gds.*"
    ports:
      - "7475:7474"  # Changed from 7474 to avoid conflicts
      - "7688:7687"  # Changed from 7687 to avoid conflicts
    volumes:
      - auth-neo4j-data:/data
      - ./neo4j/plugins:/plugins
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - auth-network
      - analytics-network

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9093:9092"  # Changed from 9092 to avoid conflicts
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - analytics-network

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    ports:
      - "2182:2181"  # Changed from 2181 to avoid conflicts
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - analytics-network

  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"  # Changed from 6379 to avoid conflicts
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data
    networks:
      - analytics-network

volumes:
  auth-postgres-data:
  auth-neo4j-data:
  redis-data:

networks:
  auth-network:
    driver: bridge
  analytics-network:
    driver: bridge


// File: query
----------------------------------------
com.docker.service


// File: README.md
----------------------------------------
# Project Roadmap Components

This repository contains the modular components of the comprehensive project roadmap. The roadmap has been broken down into smaller, manageable sections organized into the following directories:

## Main Sections

1. **Phases** - Contains detailed breakdown of each implementation phase
   - [Phase 1: Infrastructure Foundations](phases/phase1.md)
   - [Phase 2: Core Service Implementation](phases/phase2.md)
   - [Phase 3: Data Integration Layer](phases/phase3.md)
   - [Phase 4: User Interface Development](phases/phase4.md)
   - [Phase 5: Security Implementation](phases/phase5.md)
   - [Phase 6: Performance Optimization](phases/phase6.md)
   - [Phase 7: Testing & Quality Assurance](phases/phase7.md)
   - [Phase 8: Deployment Strategy](phases/phase8.md)
   - [Phase 9: Monitoring & Maintenance](phases/phase9.md)
   - [Phase 10: Documentation & Training](phases/phase10.md)
2. **Tools** - Documentation for each tool and its usage in the project
3. **Workflows** - High-level workflows and integration patterns
4. **Implementation** - Detailed implementation roadmap and timelines

## Directory Structure

```
.
├── phases/          # Detailed phase documentation
├── tools/           # Tool-specific documentation
├── workflows/       # Workflow diagrams and descriptions
├── implementation/  # Implementation timeline and milestones
└── README.md        # Main overview document
```

## How to Use

1. Start with the README.md for an overview
2. Explore specific phases in the phases/ directory
3. Refer to tools/ for tool-specific guidance
4. Check workflows/ for integration patterns
5. Review implementation/ for timeline details

// File: ROADMAP.md
----------------------------------------
Below is a comprehensive, end-to-end solution that expands upon the provided framework and demonstrates how to integrate the various AI tools, automation infrastructure, and methodological principles into a unified, maintainable system. This “final answer” weaves together all phases (1–9) and shows where each tool—Claude 3.5 Sonnet, Deepseek Coder V3, Codestal 2/M-01, O1 Pro, N8N, and MCP—fits into the overall architecture.
Master Integration Overview

    Infrastructure Foundations (Phase 1)
        Architecture & Stack:
            Microservices in a Modular Monorepo (Node.js/NestJS or Python/FastAPI for each service).
            Neo4j for graph storage.
            Docker for containerization; Kubernetes (K8s) for production orchestration.
        Development & Deployment:
            Use GitHub Actions or GitLab CI for CI/CD pipelines.
            Employ environment-specific Docker Compose/K8s manifests.
        DevOps & Environment Setup:
            Maintain .env files or a secrets manager for sensitive configs.
            Automate builds and tests on each microservice merge.

    NLP Pipeline (Phase 2)
        Initial Parsing: Basic morphological analysis, tokenization, language detection.
        Advanced NLP:
            Named Entity Recognition (NER), entity linking to external knowledge bases (e.g., Wikidata).
            Topic modeling (LDA/BERT-based) and summarization (spaCy/Hugging Face).
        Integration Flow:
            POST /parseText → returns tokens, concepts, user references.
            Store parsed data in Neo4j as Statement and Concept nodes.

    Graph Analytics & Cognitive Network Analysis (Phase 3)
        Neo4j GDS for advanced analytics (PageRank, Betweenness Centrality, node similarity, community detection).
        Insight Generation:
            Identify bridging concepts.
            Detect subgraph gaps or unconnected but semantically similar nodes.
        Scheduled GDS Jobs:
            Run nightly/weekly to update centrality measures and bridging suggestions.

    AI (LLM) Integration (Phase 4)
        Core LLM Microservice:
            Endpoints: /generateInsights, /summarizeText, /answerQuery.
            Prompt Strategy: Provide relevant subgraphs or key context to control and structure LLM outputs.
        Caching & Rate Limits:
            Use Redis to cache common queries; set usage quotas to control API costs.
        Advanced Features:
            Concept Insertion: LLM identifies new or emerging concepts.
            Conversational Agent: Chat-based UI that references the graph for Q&A.
        Tooling:
            Claude 3.5 Sonnet as the primary “high-level reasoning” LLM.
            Optionally plug in OpenAI GPT, Anthropic Claude, or on-prem open-source LLMs.

    Interactive Visualization & Collaboration (Phase 5)
        Front-End Graph Rendering:
            Cytoscape.js or Sigma.js for large-scale node/edge visualization.
            Real-time updates via WebSockets or Socket.io.
        Collaboration:
            Live multi-user environment.
            Edits to the graph reflect instantly for all connected users.
            Role-based access control (Viewer, Editor, Admin).
        User Dashboards:
            Personal statistics: contributed statements, contexts, bridging suggestions accepted/rejected.
            “Project dashboards” for domain-specific tasks or contexts.

    Extended Integrations & Scaling (Phase 6)
        External Data Sources:
            Slack/Microsoft Teams bots for real-time conversation ingestion.
            Google Docs/Drive ingestion for parsing PDFs and Word docs.
            Social media (Twitter/X, LinkedIn) or RSS feeds for topic monitoring.
        Performance & Scalability:
            Neo4j Clustering or read replicas for large graphs.
            In-memory caching (Redis/Hazelcast) to speed up repeated queries.
            Kubernetes for horizontal autoscaling of microservices.
        Monitoring & Observability:
            Prometheus/Grafana for metric collection.
            Alerts for usage spikes, memory issues, or HPC job failures.

    Methodological & Innovation Extensions (Phase 7)
        Ontological Refinement:
            Clearly document node labels, relationship definitions, property fields.
            Maintain versioned ontology for consistent cross-service usage.
        User Training & Methodology:
            Onboarding tutorials, wizard-based guidance for bridging statements, acceptance or rejection flows.
            Workshops to encourage creative cross-domain linkages.
        Creative Idea Generation:
            “Cross-domain linking” engine that automatically looks for latent opportunities between different project contexts.
            Specialized Micro-Models for domain-specific text, orchestrated with a general-purpose LLM.

    Documentation & Validation (Phase 8)
        Comprehensive Documentation:
            Technical reference for each microservice (API endpoints, data formats).
            Usage guides for data ingestion, bridging acceptance, concept management.
        Validation Framework:
            Unit/integration tests for the NLP pipeline, AI suggestions, real-time collaboration.
            Security audits for data encryption and authorization.
            Load/stress tests to ensure the system can handle large datasets and concurrency.

    Ongoing Maintenance, HPC Integration & Future Enhancements (Phase 9 & Beyond)
        HPC Routines:
            On-demand GPU clusters for heavy tasks (e.g., large-scale summarization, specialized bridging computations).
            Integrate HPC job management with MCP for resource allocation and scheduling.
        Adaptive Model Updates:
            Fine-tune domain-specific models based on user feedback.
            Maintain an internal model registry.
        Federated Knowledge Graph:
            Potentially link multiple organizations or departments with partial shared graphs.
        Quantum/Next-Gen Explorations:
            Keep an eye on future computing paradigms for advanced graph processing or LLM inference.
            Leverage our [Research Corner](research-corner/) for quantum computing experiments and HPC integration.
            Explore quantum-inspired optimization algorithms and post-quantum cryptography through isolated testing environments.

How to Leverage Each Tool
1. Claude 3.5 Sonnet

    Primary Analytical Engine
        Performs complex reasoning, high-level systematic analysis, and multi-step planning.
        Ideal for analyzing large architectural documents, formulating advanced bridging statements, and verifying overall conceptual consistency in the knowledge graph.
        Usage:
            Early-phase decomposition: Use Claude for generating initial architectural overviews and high-level designs.
            Methodological Verification: Ask Claude to compare proposed solutions (from other models) against the system’s design principles.

2. Deepseek Coder V3

    Software Development Specialist
        Deep code analysis, technical specification interpretation, architectural pattern recognition.
        Can generate or refactor large codebases, handling advanced context-aware suggestions.
        Usage:
            Writing and reviewing microservice templates.
            Generating CI/CD scripts, Dockerfiles, or helm charts for K8s.
            Recommending system optimizations or best coding practices.

3. Codestal 2/M-01

    High-Speed Code-Centric Assistant
        System architecture comprehension, pattern recognition, and code optimizations.
        Maintains context across complex files for large-scale code migrations or advanced refactoring.
        Usage:
            Rapid iterations on specific code segments.
            Bulk transformations (e.g., standardizing naming conventions, upgrading frameworks).
            High-volume tasks that require quick generation or scanning of code patterns.

4. O1 Pro

    Extreme Reasoning & Validation
        Structured technical analysis, architecture validation, and documentation cross-checking.
        Verifies assumptions, checks for consistency, and ensures system coherence.
        Usage:
            Final pass on architectural decisions (e.g., Are the chosen relationship names consistent with the overall model?).
            Validate performance or scaling plan.
            Compare statements or bridging suggestions from LLMs with established domain rules.

5. N8N Automation Platform

    Workflow Orchestration & Multi-Model Integration
        Pre- and post-processing data, chaining model calls, error handling, and monitoring.
        Usage:
            Automated triggers that take raw text from Slack → NLP Microservice → Neo4j → AI bridging suggestions → user notifications.
            Consolidate logs and maintain audit trails.
            Automatic fallback routes if an LLM times out (e.g., switch to a simpler local model).

6. MCP (Mission Control Protocol) Servers

    Distributed Computing Management
        Deploy models on HPC resources, handle load balancing, and maintain high availability.
        Usage:
            Spin up ephemeral GPU instances for specialized tasks (e.g., big-batch text summarization, advanced bridging).
            Scale in or out depending on concurrency or data volume spikes.
            Monitor resource usage to optimize cost/performance ratio.

Putting It All Together: High-Level Workflow

    User Ingestion & NLP
        A user uploads text or an automated feed from Slack/Docs triggers ingestion (managed by N8N).
        The text is sent to Claude 3.5 Sonnet for an initial overview or to the NLP Microservice for morphological parsing and concept extraction.
        Entities and statements are created/updated in Neo4j.

    Graph Updates & Analytics
        A background job runs on a schedule (managed by N8N or cron in K8s) to compute GDS algorithms (PageRank, community detection).
        O1 Pro periodically validates the new subgraphs for structural consistency.

    AI-Driven Insight Generation
        When bridging suggestions are needed, the relevant subgraph is fetched.
        Claude 3.5 Sonnet or a fine-tuned LLM (managed by MCP) proposes bridging concepts or link statements.
        The user reviews and either approves or rejects the suggestions.

    Collaborative Visualization
        The front-end visualizes the knowledge graph (Cytoscape.js or Sigma.js).
        Users can drag/drop nodes, add statements, or create relationships in real-time.
        WebSockets (Socket.io) broadcast these edits to all collaborators.

    Continuous Code & Infrastructure Evolution
        Deepseek Coder V3 and Codestal 2/M-01 maintain the microservice codebases, Dockerfiles, and integration scripts.
        CI/CD pipelines run tests, building new images and deploying them to K8s via MCP servers.
        O1 Pro cross-checks architectural changes and documentation updates.

Example Implementation Roadmap

    Project Kickoff (Weeks 1–2)
        Set up Monorepo structure (e.g., Nx or Turborepo).
        Initialize microservices (Auth service, NLP service, Graph service).
        Containerize with Docker; set up minimal CI pipeline.

    Core Graph & NLP (Weeks 3–8)
        Implement the NLP Microservice with language detection, tokenization, morphological analysis.
        Configure Neo4j schema (Concept, Statement, Context, User).
        Use Deepseek Coder V3 to generate robust code and unit tests for each piece.

    Graph Analytics & Basic Insights (Weeks 9–14)
        Install Neo4j GDS, run PageRank/centrality metrics.
        Create an “Insight Microservice” to call GDS results and store them in the graph.
        Use O1 Pro to validate the new analytics flow and compare results with known edge cases.

    LLM Integration (Weeks 15–20)
        Stand up an LLM Microservice (likely Node/Python) that connects to a provider (OpenAI, Anthropic, or on-prem).
        Integrate with the graph: bridging concept requests → LLM prompt → results → user acceptance.
        Leverage Claude 3.5 Sonnet or your LLM of choice for summarization/insight generation.

    Collaboration & Visualization (Weeks 21–26)
        Build a front-end (React/Angular/Vue) with Cytoscape.js or Sigma.js.
        Implement real-time collaboration via Socket.io.
        Deploy an initial user role/permission system.

    Extended Integrations & HPC (Weeks 27–32)
        Connect Slack bots, Google Docs ingestion, or other external feeds.
        Deploy HPC resources (GPUs, large memory nodes) managed by MCP.
        Schedule advanced tasks (summarization, bridging runs on large corpora) via HPC.

    Methodology & Documentation (Continuous + Weeks 33–40)
        Refine the internal ontology, maintain a “living protocol” for node/relationship definitions.
        Provide user tutorials and admin guides in the platform’s documentation hub.
        Use O1 Pro to cross-check correctness and to finalize design decisions.

    Validation, QA, and Continuous Maintenance (Ongoing)
        Automate load tests, security scans, and HPC resource checks.
        Roll out new features or domain-specific expansions (healthcare, finance) with specialized models.
        Incorporate user feedback loops to fine-tune bridging thresholds and AI suggestions.

Key Takeaways & Best Practices

    Synergize the Tools
        Claude 3.5 Sonnet for high-level reasoning and bridging concept generation.
        Deepseek Coder V3 for deeper code generation and architectural enhancements.
        Codestal 2/M-01 for large-scale refactoring, quick code transformations, and pattern recognition.
        O1 Pro for methodical checks, advanced reasoning, and final verification of system coherence.

    Automate Everything
        Use N8N for orchestrating workflows across NLP, Graph, LLM microservices, and HPC triggers.
        Keep your CI/CD pipelines robust, with automatic tests and container builds.

    Document as You Go
        Maintain consistent naming conventions.
        Provide clear instructions for each microservice’s API endpoints and data exchange formats.
        Update users on any major schema changes or new bridging features.

    Focus on User Validation
        Bridging suggestions and new concept insertions should always have a human-in-the-loop step.
        Track acceptance/rejection to refine model behavior over time.

    Plan for Evolution
        The system should be modular enough to swap in new LLMs or HPC backends.
        Regularly revisit your ontology, especially if you add new domains or external knowledge bases.

Concluding Vision

By following this expanded roadmap:

    You establish a robust data ingestion pipeline (NLP microservice, knowledge graph schema).
    Leverage advanced graph analytics (Neo4j GDS) to reveal relationships and potential knowledge gaps.
    Integrate powerful LLM services (Claude 3.5 Sonnet or others) for summarization, bridging, and creative generation.
    Visualize everything in real time to foster collaborative idea-building and alignment within teams.
    Scale both computationally (HPC, GPU clusters) and organizationally (federated knowledge graphs, domain expansions) as your dataset and user base grow.

The final product is a living cognitive ecosystem where human insight and AI-driven analytics amplify each other, continuously pushing the boundaries of organizational or research knowledge management.

// File: implementation\future-innovations.md
----------------------------------------
# Future Innovations & Creative Extensions

This document outlines innovative expansions and creative possibilities for the auth-service and overall system architecture, based on our current foundation.

## 1. Graph-Based Authentication & Authorization

### Dynamic RBAC with Neo4j
- Store user roles and permissions as graph relationships
- Enable complex permission queries:
  ```cypher
  MATCH (u:User)-[r:HAS_ACCESS]->(d:Domain)
  WHERE u.id = $userId
  RETURN d.name, r.accessLevel
  ```
- Implement community-based access patterns
- Track permission inheritance and conflicts through graph traversal

### Advanced User Analytics
- Identify key users bridging different domains
- Analyze access patterns for security anomalies
- Query examples:
  ```cypher
  // Find users bridging multiple domains
  MATCH (u:User)-[:HAS_ACCESS]->(d1:Domain)
  MATCH (u)-[:HAS_ACCESS]->(d2:Domain)
  WHERE d1 <> d2
  RETURN u.name, count(DISTINCT d1) as domains
  ORDER BY domains DESC
  ```

## 2. AI-Driven Code Architecture

### Automated Code Review Pipeline
- Integrate LLMs for code review on each PR
- Analysis areas:
  - Security best practices
  - Performance optimizations
  - Documentation completeness
  - Architectural consistency

### Smart Documentation Generation
- Parse markdown files into knowledge graphs
- Generate visual documentation maps
- Identify documentation gaps
- Auto-update docs based on code changes

### AI-Assisted Testing
- Generate test cases based on code changes
- Identify edge cases through static analysis
- Suggest integration test scenarios

## 3. HPC & GPU Integration

### GPU-Accelerated Auth Flows
- Implement parallel processing for bulk operations
- Use GPU acceleration for:
  - Token validation
  - Cryptographic operations
  - Pattern matching in access logs

### HPC Job Queue Integration
- Define job templates for resource-intensive tasks
- Implement priority-based scheduling
- Monitor resource utilization

## 4. Real-Time Visualization & Monitoring

### User Session Visualization
- Real-time graph of active sessions
- Visual representation of permission changes
- Interactive exploration of user relationships

### System Health Dashboard
- Container health metrics
- Database performance visualization
- Auth flow success/failure rates

## 5. Advanced Security Features

### AI-Powered Threat Detection
- Analyze access patterns for anomalies
- Predict potential security risks
- Automated incident response

### Zero-Trust Implementation
- Continuous authentication
- Context-aware access decisions
- Real-time risk assessment

## 6. Knowledge Graph Integration

### Automated Documentation Analysis
- Convert documentation to graph structure
- Track relationships between components
- Identify implementation gaps

### Code-Doc Alignment
- Verify documentation accuracy
- Track component dependencies
- Generate architecture diagrams

## Implementation Timeline

### Phase 1: Foundation (Current)
- ✓ Basic auth service
- ✓ PostgreSQL & Neo4j integration
- ✓ Docker containerization

### Phase 2: Graph Enhancement
- Implement graph-based RBAC
- Add user analytics queries
- Develop visualization prototypes

### Phase 3: AI Integration
- Set up code review pipeline
- Implement documentation analysis
- Deploy test generation system

### Phase 4: HPC & Performance
- Configure GPU resources
- Implement job queuing
- Optimize auth flows

### Phase 5: Advanced Features
- Deploy threat detection
- Implement zero-trust architecture
- Enhance visualization tools

## Getting Started

To begin exploring these innovations:

1. Start with graph-based auth:
   ```bash
   # Update Neo4j schema
   npm run migrate:graph
   
   # Generate test data
   npm run seed:graph-auth
   ```

2. Enable AI code review:
   ```bash
   # Install dependencies
   npm install @ai/code-review
   
   # Configure GitHub webhook
   npm run configure:ai-review
   ```

3. Set up HPC integration:
   ```bash
   # Configure HPC connection
   npm run setup:hpc
   
   # Test GPU availability
   npm run test:gpu
   ```

## Contributing

When implementing these features:

1. Start small - implement one feature at a time
2. Write comprehensive tests
3. Document architectural decisions
4. Consider performance implications
5. Maintain security best practices

## Next Steps

1. Review current architecture for integration points
2. Prototype graph-based auth implementation
3. Set up development environment for GPU testing
4. Begin documentation knowledge graph conversion
5. Plan AI pipeline integration

Remember: These innovations should enhance, not complicate, the core authentication service. Implement gradually and validate each step.

// File: implementation\phase1-timeline.md
----------------------------------------
# Phase 1 Implementation Timeline

## Infrastructure Setup (January 20, 2025)

### Authentication Service Setup

#### Initial Service Structure
- Created basic NestJS service structure
- Implemented configuration management with environment validation
- Set up TypeORM and Neo4j database connections
- Added health check endpoint for monitoring

#### Health Check Implementation
1. Created health module with:
   - `HealthController` for endpoint routing
   - `HealthService` for database connectivity checks
   - Comprehensive test coverage

2. Database Integration:
   - PostgreSQL for structured data
   - Neo4j for graph relationships
   - Health checks for both databases

#### Configuration Management
1. Environment Configuration:
   - Development environment (.env)
   - Test environment (.env.test)
   - Production environment (Docker)

2. Validation Schema:
   ```typescript
   validationSchema: Joi.object({
     DB_HOST: Joi.string().required(),
     DB_PORT: Joi.number().default(5432),
     DB_USER: Joi.string().required(),
     DB_PASSWORD: Joi.string().required(),
     DB_NAME: Joi.string().required(),
     NEO4J_URI: Joi.string().required(),
     NEO4J_USER: Joi.string().required(),
     NEO4J_PASSWORD: Joi.string().required(),
     PORT: Joi.number().default(3000),
     NODE_ENV: Joi.string()
       .valid('development', 'production', 'test')
       .default('development'),
   })
   ```

#### Docker Setup
1. Development Environment:
   - PostgreSQL container
   - Neo4j container
   - Auth service container
   - Health checks for all services

2. Test Environment:
   ```yaml
   services:
     postgres:
       image: postgres:13-alpine
       environment:
         POSTGRES_USER: test
         POSTGRES_PASSWORD: test
         POSTGRES_DB: test_auth
       healthcheck:
         test: ['CMD-SHELL', 'pg_isready -U test']
         interval: 5s
         timeout: 5s
         retries: 5

     neo4j:
       image: neo4j:4.4
       environment:
         NEO4J_AUTH: neo4j/test
         NEO4J_ACCEPT_LICENSE_AGREEMENT: 'yes'
       ports:
         - '7474:7474'
         - '7687:7687'
       healthcheck:
         test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
         interval: 10s
         timeout: 5s
         retries: 5
         start_period: 15s
   ```

#### Testing Infrastructure
1. Test Setup:
   - Jest configuration
   - Test environment configuration
   - Database test containers
   - Health check test suite

2. Test Coverage:
   - Unit tests for health service
   - Integration tests for database connections
   - End-to-end tests for health endpoint

### Next Steps
1. Authentication Implementation:
   - User model and schema
   - Authentication endpoints
   - JWT integration
   - Role-based access control

2. Service Integration:
   - API gateway setup
   - Service discovery
   - Load balancing configuration

## Related Documentation
- [Docker Setup](../tools/docker.md)
- [CI/CD Pipeline](../workflows/cicd.md)
- [Phase 1 Architecture](../phases/phase1.md)

// File: implementation\phase2-implementation.md
----------------------------------------
# Phase 2: Core Service Implementation Progress

## Current Status (January 20, 2025)

### Completed Components

#### 1. Authentication Service Base Structure
- ✓ Basic NestJS service setup
- ✓ Health check endpoints
- ✓ Configuration management
- ✓ Database connection (TypeORM + PostgreSQL)

#### 2. User Management System
- ✓ User entity with role-based access
- ✓ CRUD operations for users
- ✓ Input validation with DTOs
- ✓ Password hashing preparation
- ✓ Role-based authorization guards

### In Progress

#### 1. Authentication Module
- JWT strategy implementation
- OAuth2 integration
- Token management
- Session handling

#### 2. Security Features
- Rate limiting
- Request validation
- CORS configuration
- Security headers

### Next Steps

1. Authentication Implementation
   - Create JWT strategy
   - Implement login/logout flow
   - Add token refresh mechanism
   - Set up OAuth2 providers

2. API Gateway Setup
   - Initialize gateway service
   - Configure GraphQL
   - Set up service discovery

3. Data Processing Pipeline
   - Create NLP service structure
   - Set up Python environment
   - Implement core processing features

4. AI Service Integration
   - Configure DeepSeek client with separate API endpoint
   ```typescript
   @Injectable()
   export class DeepseekService {
     private readonly client: OpenAI;
     
     constructor(private configService: ConfigService) {
       this.client = new OpenAI({
         apiKey: this.configService.get('DEEPSEEK_API_KEY'),
         baseURL: 'https://api.deepseek.com/v1',
       });
     }

     async chatCompletion(messages: Array<ChatMessage>) {
       return this.client.chat.completions.create({
         model: 'deepseek-reasoner',
         messages: messages.filter(msg => !msg.reasoning_content),
       });
     }
   }
   ```
   - Implement message history management excluding reasoning_content
   - Add CoT storage handling for debug/analytics
   - Update environment configuration with DEEPSEEK_API_KEY
# Phase 2: Core Service Implementation Progress

## Current Status (January 20, 2025)

### Completed Components

#### 1. Authentication Service Base Structure
- ✓ Basic NestJS service setup
- ✓ Health check endpoints
- ✓ Configuration management
- ✓ Database connection (TypeORM + PostgreSQL)

#### 2. User Management System
- ✓ User entity with role-based access
- ✓ CRUD operations for users
- ✓ Input validation with DTOs
- ✓ Password hashing preparation
- ✓ Role-based authorization guards

### In Progress

#### 1. Authentication Module
- JWT strategy implementation
- OAuth2 integration
- Token management
- Session handling

#### 2. Security Features
- Rate limiting
- Request validation
- CORS configuration
- Security headers

### Next Steps

1. Authentication Implementation
   - Create JWT strategy
   - Implement login/logout flow
   - Add token refresh mechanism
   - Set up OAuth2 providers

2. API Gateway Setup
   - Initialize gateway service
   - Configure GraphQL
   - Set up service discovery

3. Data Processing Pipeline
   - Create NLP service structure
   - Set up Python environment
   - Implement core processing features

## Testing Strategy

### Implemented
- ✓ Basic health check tests
- ✓ Configuration validation

### Pending
- User management tests
- Authentication flow tests
- Integration tests
- Security tests

## Documentation Status

### Completed
- ✓ Basic service structure
- ✓ User management API
- ✓ Environment configuration

### In Progress
- Authentication flows
- API specifications
- Security guidelines

## Timeline

1. Authentication Module (Week 1-2)
   - JWT implementation
   - OAuth2 setup
   - Testing & documentation

2. API Gateway (Week 2-3)
   - Service setup
   - GraphQL implementation
   - Service integration

3. Data Processing (Week 3-4)
   - Pipeline setup
   - Core processing features
   - Integration with other services

## Risks and Mitigations

1. Security
   - Implementing comprehensive security testing
   - Regular dependency updates
   - Code review processes

2. Scalability
   - Database optimization
   - Caching strategies
   - Load testing

3. Integration
   - Clear API contracts
   - Comprehensive integration tests
   - Service monitoring

## Next Actions
1. Begin JWT strategy implementation
2. Set up authentication module structure
3. Create login/logout endpoints
4. Implement token management


// File: implementation\phase3-implementation.md
----------------------------------------
# Phase 3: Data Integration Layer Implementation

## Overview
The data integration layer provides robust data processing, analytics, and retention capabilities for the VAIM2 platform. This phase builds upon the existing infrastructure to add automated data pipeline management, monitoring, and data lifecycle controls.

## Components Implemented

### 1. Data Pipeline Management
- **Pipeline Service**
  - Automated analytics scheduling
  - Configurable job execution
  - Integration with monitoring systems
  - Error handling and recovery
- **Job Types**
  - Daily analytics (PageRank, community detection)
  - Weekly analytics (node similarity, graph statistics)
  - Data retention enforcement

### 2. Monitoring System
- **Prometheus Integration**
  - Custom metrics collection
  - Performance monitoring
  - Job execution tracking
  - Data retention metrics
- **Alerting Capabilities**
  - Job failure notifications
  - Performance degradation alerts
  - Data retention warnings

### 3. Data Retention Management
- **Retention Policies**
  - Configurable retention periods
  - Automated data cleanup
  - Secure archival process
- **Data Lifecycle**
  - Age-based retention rules
  - Archive management
  - Audit logging

### 4. Security Enhancements
- **OAuth2 Integration**
  - Multiple provider support (Google, GitHub)
  - Enhanced profile fetching
  - Robust error handling
- **Token Management**
  - Secure token storage
  - Refresh token rotation
  - Token blacklisting

## Configuration
- Environment-specific settings
- Configurable schedules
- Retention policy parameters
- Monitoring thresholds

## Documentation
- [Data Pipeline Documentation](../services/graph-analytics-service/docs/data-pipeline.md)
- [OAuth2 Implementation](../services/auth-service/docs/oauth2-implementation.md)

## Testing
- Unit tests for new components
- Integration tests for pipeline flows
- Security testing for OAuth2
- Performance testing for analytics jobs

## Deployment
- Docker container updates
- Environment configuration
- Monitoring setup
- Pipeline scheduling

## Next Steps
1. Integration with external data sources
2. Enhanced analytics capabilities
3. Machine learning pipeline integration
4. Federated data management

## Technical Debt & Improvements
- [ ] Add retry mechanisms for failed jobs
- [ ] Implement more granular monitoring
- [ ] Enhance archival compression
- [ ] Add more OAuth2 providers

## Security Considerations
1. Data encryption in transit and at rest
2. Access control for analytics jobs
3. Secure token management
4. Audit logging for all operations

## Performance Optimizations
1. Batch processing for large datasets
2. Caching for frequent operations
3. Efficient data archival process
4. Optimized query patterns

This implementation completes Phase 3 of the VAIM2 platform, providing a robust foundation for data management, security, and analytics processing. The system is now ready for the next phase of development, focusing on advanced analytics and machine learning integration.

// File: implementation\phase4-implementation.md
----------------------------------------
# Phase 4: AI/LLM Integration Implementation

## Overview
The AI/LLM integration layer provides a unified interface for accessing various LLM providers through a single API. The implementation supports both direct provider access and OpenRouter integration for accessing multiple models through a single endpoint.

## Components Implemented

### 1. LLM Service Core
- **Provider Abstraction**
  - Provider-agnostic interface
  - Multiple provider support
  - Automatic fallback handling
  - Health monitoring
- **Model Support**
  - DeepSeek R1 and Chat
  - Claude 3.5 Sonnet (via OpenRouter)
  - Other models via OpenRouter

### 2. API Layer
- **GraphQL API**
  - Query-based completions
  - Streaming subscriptions
  - Model and provider listing
  - Health checks
- **REST API**
  - Standard completion endpoints
  - SSE streaming support
  - Provider management
  - Health monitoring

### 3. Caching System
- **Redis Integration**
  - Response caching
  - Rate limiting
  - Token usage tracking
  - Health monitoring

### 4. Infrastructure
- **Service Architecture**
  - NestJS-based microservice
  - GraphQL and REST support
  - WebSocket integration
  - Health monitoring
- **Docker Support**
  - Multi-stage builds
  - Development configuration
  - Production optimization

## Configuration
- Environment-specific settings
- Provider API keys
- Caching parameters
- Rate limiting rules
- Monitoring thresholds

## API Documentation

### GraphQL Endpoints
```graphql
type Query {
  complete(messages: [ChatMessageInput!]!, options: CompletionOptionsInput): CompletionResponse!
  listProviders: [String!]!
  listModels(provider: String!): [String!]!
}

type Mutation {
  startStreamCompletion(messages: [ChatMessageInput!]!, options: CompletionOptionsInput): Boolean!
}

type Subscription {
  streamCompletion(streamId: String!): StreamCompletionResponse!
}
```

### REST Endpoints
- `POST /api/v1/llm/complete` - Text completion
- `GET /api/v1/llm/complete/stream` - Streaming completion
- `GET /api/v1/llm/providers` - List providers
- `GET /api/v1/llm/providers/:provider/models` - List models
- `GET /api/v1/health` - Health check

## Next Steps

### 1. Integration Testing
- [ ] End-to-end provider tests
- [ ] Streaming performance tests
- [ ] Load testing
- [ ] Error handling verification

### 2. Monitoring Enhancement
- [ ] Detailed usage metrics
- [ ] Cost tracking
- [ ] Performance monitoring
- [ ] Error rate tracking

### 3. Provider Expansion
- [ ] Additional model support
- [ ] Provider-specific optimizations
- [ ] Custom model integration
- [ ] Fallback strategy refinement

### 4. Documentation
- [ ] API documentation
- [ ] Integration guides
- [ ] Configuration reference
- [ ] Best practices

## Technical Debt & Improvements
- [ ] Enhanced error handling
- [ ] More granular caching
- [ ] Better token estimation
- [ ] Cost optimization
- [ ] Performance tuning

## Security Considerations
1. API key management
2. Rate limiting
3. Input validation
4. Output filtering
5. Token usage monitoring

## Performance Optimizations
1. Response caching
2. Request batching
3. Connection pooling
4. Resource management

This implementation establishes the foundation for AI/LLM integration in the VAIM2 platform. The service provides a robust, scalable interface for accessing various LLM providers while maintaining flexibility for future expansions and optimizations.

// File: implementation\roadmap.md
----------------------------------------
# Implementation Roadmap

## Project Kickoff (Weeks 1-2)
- Monorepo structure setup
- Initial microservices creation:
  - Auth service
  - NLP service
  - Graph service
- Containerization with Docker
- Basic CI pipeline setup

## Core Graph & NLP (Weeks 3-8)
- NLP Microservice implementation:
  - Language detection
  - Tokenization
  - Morphological analysis
- Neo4j schema configuration
- Unit test development

## Graph Analytics & Insights (Weeks 9-14)
- Neo4j GDS installation
- Analytics implementation:
  - PageRank
  - Centrality metrics
  - Community detection
- Insight Microservice creation
- Analytics flow validation

## LLM Integration (Weeks 15-20)
- LLM Microservice setup
- Graph integration:
  - Bridging concept requests
  - LLM prompt handling
  - User acceptance flow
- Summarization capabilities

## Collaboration & Visualization (Weeks 21-26)
- Front-end development
- Real-time collaboration
- User role/permission system

## Extended Integrations (Weeks 27-32)
- External integrations:
  - Slack bots
  - Google Docs ingestion
- HPC resource deployment
- Advanced task scheduling

## Methodology & Documentation (Weeks 33-40)
- Ontology refinement
- User tutorials
- Admin guides
- Design validation

## Ongoing Maintenance
- Automated testing
- Security scans
- HPC resource monitoring
- User feedback integration

// File: phases\phase1.md
----------------------------------------
# Phase 1: Infrastructure Foundations

## Architecture & Stack

### Microservices Architecture
- **Monorepo Structure**
  - Shared libraries and utilities
  - Centralized dependency management
  - Example structure:
    ```
    /services
      /auth-service
      /data-service
      /api-gateway
    /libs
      /common
      /config
    ```

- **Service Implementation**
  - Node.js/NestJS for API services
  - Python/FastAPI for data processing services
  - gRPC for inter-service communication

### Data Storage
- **Graph Database**
  - Neo4j for relationship-heavy data
  - Cypher query language
  - Example schema:
    ```cypher
    (User)-[:HAS_PROFILE]->(Profile)
    (User)-[:CREATED]->(Content)
    ```

- **Relational Database**
  - PostgreSQL for structured data
  - Prisma ORM for Node.js services

### Containerization & Orchestration  
- **Development**
  - Docker Compose for local development
  - Example compose file:
    ```yaml
    version: '3'
    services:
      auth-service:
        build: ./services/auth-service
        ports:
          - "3001:3000"
    ```

- **Production**
  - Kubernetes cluster setup
  - Helm charts for deployment
  - Ingress controller configuration

## Development & Deployment

### CI/CD Pipelines
- **GitHub Actions Workflow**
  - Linting and testing on PR
  - Build and push Docker images
  - Deployment to staging environment

- **Release Process**
  - Semantic versioning
  - Changelog generation
  - Automated release notes

### Environment Management
- **Environment Variables**
  - .env files for local development
  - Kubernetes secrets for production
  - Example secret manifest:
    ```yaml
    apiVersion: v1
    kind: Secret
    metadata:
      name: db-credentials
    type: Opaque
    data:
      DB_USER: base64encoded
      DB_PASS: base64encoded
    ```

## DevOps & Environment Setup

### Configuration Management
- **Centralized Configuration**
  - Config service for shared settings
  - Environment-specific overrides
  - Example config structure:
    ```typescript
    export default {
      database: {
        host: process.env.DB_HOST,
        port: parseInt(process.env.DB_PORT),
      }
    }
    ```

### Automation
- **Infrastructure as Code**
  - Terraform for cloud resources
  - Ansible for server provisioning
  - Example Terraform resource:
    ```hcl
    resource "aws_instance" "web" {
      ami           = "ami-123456"
      instance_type = "t2.micro"
    }
    ```

- **Monitoring Setup**
  - Prometheus for metrics collection
  - Grafana dashboards
  - Alert manager configuration

## Key Considerations

### Scalability
- Horizontal scaling of services
- Database sharding strategies
- Caching layers (Redis)

### Security
- Authentication/Authorization
  - JWT tokens
  - Role-based access control
- Network security
  - VPC configuration
  - Firewall rules

### Documentation Standards
- API documentation (OpenAPI/Swagger)
- Architecture decision records (ADRs)
- Service-level documentation
  - Service boundaries
  - Data models
  - API contracts

## Related Documentation
- [Tools: Docker Setup](tools/docker.md)
- [Workflows: CI/CD Pipeline](workflows/cicd.md)
- [Implementation: Phase 1 Timeline](implementation/phase1-timeline.md)

// File: phases\phase10.md
----------------------------------------
# Phase 10: Documentation & Training

## Architecture & Stack

- **Documentation Components**
  - Technical documentation
  - API documentation
  - User guides
- **Technology Choices**
  - MkDocs for documentation
  - Swagger/OpenAPI for API docs
  - Video tutorials for training

## Development & Deployment

- **Documentation Strategy**
  - Living documentation approach
  - Version-controlled documentation
  - Automated documentation generation
- **Training Implementation**
  - Onboarding materials
  - Technical workshops
  - Knowledge base creation

## DevOps & Environment Setup

- **Documentation Configuration**
  - Documentation hosting
  - Search functionality
  - Access control
- **Training**
  - Training environment setup
  - Demo data preparation
  - Feedback collection

## Key Considerations

- Documentation maintainability
- Training effectiveness
- Knowledge transfer
- Continuous improvement

// File: phases\phase2.md
----------------------------------------
# Phase 2: Core Service Implementation

## Architecture & Stack

- **Service Components**
  - Authentication service
  - Data processing pipeline
  - API gateway
- **Technology Choices**
  - Node.js/NestJS for API services
  - Python for data processing
  - GraphQL for API gateway

## Development & Deployment

- **Service Implementation**
  - Authentication service with OAuth2
  - Data processing pipeline with ETL capabilities
  - API gateway for service orchestration
- **AI Integration**
  - Implement DeepSeek Reasoner for transparent reasoning chains
  - Add Chain-of-Thought (CoT) handling in API responses
  - Configure separate OpenAI client for DeepSeek endpoints
  - Implement message history management excluding CoT content
- **Testing Strategy**
# Phase 2: Core Service Implementation

## Architecture & Stack

- **Service Components**
  - Authentication service
  - Data processing pipeline
  - API gateway
- **Technology Choices**
  - Node.js/NestJS for API services
  - Python for data processing
  - GraphQL for API gateway

## Development & Deployment

- **Service Implementation**
  - Authentication service with OAuth2
  - Data processing pipeline with ETL capabilities
  - API gateway for service orchestration
- **Testing Strategy**
  - Unit tests for each service
  - Integration tests for service interactions

## DevOps & Environment Setup

- **Service Configuration**
  - Centralized configuration management
  - Environment-specific service configurations
- **Monitoring**
  - Service health monitoring
  - Performance metrics collection

## Key Considerations

- Security implementation for authentication
- Data validation and sanitization
- Service discovery and load balancing
- API versioning strategy


// File: phases\phase3.md
----------------------------------------
# Phase 3: Data Integration Layer

## Architecture & Stack

- **Data Integration Components**
  - Data ingestion pipeline
  - Data transformation services
  - Data storage integration
- **Technology Choices**
  - Apache Kafka for event streaming
  - Apache Spark for data processing
  - Data warehouse integration (Snowflake/BigQuery)

## Development & Deployment

- **Pipeline Implementation**
  - Real-time data ingestion
  - Batch processing capabilities
  - Data quality monitoring
- **Integration Strategy**
  - API-based data access
  - Event-driven architecture
  - Data versioning and lineage

## DevOps & Environment Setup

- **Data Pipeline Configuration**
  - Environment-specific data sources
  - Pipeline monitoring and alerting
  - Data retention policies
- **Security**
  - Data encryption in transit and at rest
  - Access control mechanisms
    - OAuth2 integration for secure third-party authentication
    - Role-based access control (RBAC)
    - Fine-grained permissions for data access
  - Audit logging
    - Authentication events
    - Data access tracking
    - System changes
  - Token management
    - JWT token lifecycle
    - Refresh token rotation
    - Token blacklisting
  - Provider integration
    - Multiple OAuth2 provider support
    - Provider-specific security configurations
    - User profile mapping and validation

## Key Considerations

- Data consistency and integrity
- Scalability of data processing
- Real-time vs batch processing tradeoffs
- Data governance and compliance

// File: phases\phase4.md
----------------------------------------
# Phase 4: AI/LLM Integration

## Architecture & Stack

- **LLM Microservice**
  - Node.js/NestJS backend
  - Redis for caching
  - Provider-agnostic design
  - Rate limiting and quotas

- **API Integration**
  - GraphQL API design
  - REST API standardization
  - WebSocket infrastructure
  - Real-time updates

## Development & Deployment

- **AI Components**
  - Insight generation service
  - Text summarization
  - Query processing
  - Concept identification
  - Bridging suggestions

- **Integration Features**
  - Caching strategies
  - Rate limiting
  - Error handling
  - Performance optimization

## DevOps & Environment Setup

- **Infrastructure**
  - LLM provider configuration
  - Redis caching layer
  - Monitoring and logging
  - Performance metrics

- **Security**
  - Rate limiting
  - Input validation
  - Output filtering
  - Usage monitoring

## Key Considerations

- Provider-agnostic architecture
- Scalable processing pipeline
- Efficient caching strategies
- Real-time capabilities
- Cost optimization
- Error handling and fallbacks

// File: phases\phase5.md
----------------------------------------
# Phase 5: Interactive Visualization & User Interface Development

## Architecture & Stack

- **Frontend Framework**
  - React.js with TypeScript
  - State management (Redux/Recoil)
  - Component library (Material-UI/TailwindCSS)

- **Visualization Engine**
  - Cytoscape.js/Sigma.js for graph visualization
  - D3.js for data visualization
  - WebGL rendering for large datasets

- **Real-time Integration**
  - WebSocket connections
  - Socket.io for real-time updates
  - State synchronization

## Development & Deployment

- **UI Components**
  - Reusable component library
  - Storybook for component documentation
  - Accessibility compliance (WCAG 2.1)
  - Responsive design patterns

- **Testing Strategy**
  - Unit tests with Jest
  - Integration tests with Cypress
  - Visual regression testing
  - Performance benchmarking

- **Collaboration Features**
  - Multi-user editing
  - Real-time updates
  - Conflict resolution
  - User presence indicators

## DevOps & Environment Setup

- **Build Pipeline**
  - Webpack configuration
  - Code splitting and lazy loading
  - CI/CD integration
  - Environment-specific builds

- **Performance Optimization**
  - Bundle size monitoring
  - Caching strategies
  - CDN integration
  - Load time optimization

## Key Considerations

- **User Experience**
  - Responsive design implementation
  - Internationalization support
  - Progressive Web App capabilities
  - Browser compatibility
  - Mobile-first approach

- **Performance**
  - Large graph rendering optimization
  - Data streaming for large datasets
  - Memory management
  - Offline capabilities

- **Security**
  - Client-side security measures
  - Data encryption
  - Input validation
  - XSS prevention

- **Accessibility**
  - Screen reader support
  - Keyboard navigation
  - Color contrast compliance
  - Focus management

// File: phases\phase6.md
----------------------------------------
# Phase 6: Performance Optimization

## Architecture & Stack

- **Performance Components**
  - Caching layer (Redis/Memcached)
  - Content Delivery Network (CDN)
  - Database optimization tools
- **Technology Choices**
  - Redis for caching
  - Cloudflare/Akamai for CDN
  - Query optimization tools

## Development & Deployment

- **Optimization Strategies**
  - Database indexing and query optimization
  - Application-level caching
  - Asynchronous processing
- **Testing Strategy**
  - Load testing
  - Stress testing
  - Performance monitoring

## DevOps & Environment Setup

- **Performance Configuration**
  - Auto-scaling configuration
  - Resource allocation optimization
  - Caching strategies
- **Monitoring**
  - Application performance monitoring
  - Resource utilization tracking
  - Alerting for performance thresholds

## Key Considerations

- Latency reduction
- Throughput optimization
- Resource efficiency
- Cost-performance tradeoffs

// File: phases\phase7.md
----------------------------------------
# Phase 7: Testing & Quality Assurance

## Architecture & Stack

- **Testing Components**
  - Unit testing framework
  - Integration testing tools
  - End-to-end testing platform
- **Technology Choices**
  - Jest for unit tests
  - Cypress for E2E tests
  - Postman for API testing

## Development & Deployment

- **Testing Strategy**
  - Test pyramid implementation
  - Continuous testing integration
  - Test automation framework
- **Quality Assurance**
  - Code coverage requirements
  - Static code analysis
  - Code review process

## DevOps & Environment Setup

- **Testing Configuration**
  - Test environment management
  - Test data management
  - CI/CD pipeline integration
- **Monitoring**
  - Test result reporting
  - Test coverage tracking
  - Quality metrics dashboard

## Key Considerations

- Test maintainability
- Test data management
- Environment consistency
- Test execution performance

// File: phases\phase8.md
----------------------------------------
# Phase 8: Deployment Strategy

## Architecture & Stack

- **Deployment Components**
  - Container orchestration
  - Infrastructure as Code
  - Deployment pipelines
- **Technology Choices**
  - Kubernetes for orchestration
  - Terraform for infrastructure
  - ArgoCD for GitOps

## Development & Deployment

- **Deployment Patterns**
  - Blue-green deployments
  - Canary releases
  - Feature flag management
- **Strategy Implementation**
  - Zero-downtime deployments
  - Rollback mechanisms
  - Environment promotion

## DevOps & Environment Setup

- **Deployment Configuration**
  - Environment-specific configurations
  - Secret management
  - Resource provisioning
- **Monitoring**
  - Deployment health checks
  - Rollout status tracking
  - Post-deployment verification

## Key Considerations

- Deployment reliability
- Rollback safety
- Environment consistency
- Deployment automation

// File: phases\phase9.md
----------------------------------------
# Phase 9: Monitoring & Maintenance

## Architecture & Stack

- **Monitoring Components**
  - Application performance monitoring
  - Infrastructure monitoring
  - Log management
- **Technology Choices**
  - Prometheus/Grafana for metrics
  - ELK stack for logging
  - PagerDuty for alerting

## Development & Deployment

- **Monitoring Implementation**
  - Custom metrics instrumentation
  - Log aggregation strategy
  - Alerting configuration
- **Maintenance Strategy**
  - Regular system updates
  - Security patching
  - Capacity planning

## DevOps & Environment Setup

- **Monitoring Configuration**
  - Dashboard creation
  - Alert thresholds
  - Retention policies
- **Maintenance**
  - Scheduled maintenance windows
  - Automated patching
  - Backup strategies

## Key Considerations

- Monitoring coverage
- Alert fatigue prevention
- Maintenance scheduling
- Disaster recovery planning

// File: phases\quantum-readiness.md
----------------------------------------
# Quantum Computing Readiness Strategy

This document outlines our approach to preparing for quantum computing capabilities and high-performance computing (HPC) integration within our system.

## Overview

As quantum computing continues to evolve, we're taking proactive steps to ensure our system is ready to leverage these advanced computational capabilities. Our strategy involves creating isolated environments for experimentation while maintaining the ability to integrate quantum-inspired algorithms and post-quantum cryptography into our production systems when appropriate.

## Research Corner

We've established a dedicated [Research Corner](../research-corner) for quantum computing and HPC experimentation. This environment serves as our sandbox for:

- Testing quantum-inspired optimization algorithms
- Experimenting with post-quantum cryptography
- Exploring HPC integration patterns
- Evaluating quantum simulation frameworks

## Strategic Phases

### Phase 1: Foundation (Current)
- Establish isolated research environment
- Set up basic quantum experiment infrastructure
- Define integration patterns for future quantum capabilities

### Phase 2: Quantum-Inspired Algorithms
- Implement classical simulations of quantum algorithms
- Test quantum-inspired optimization techniques
- Evaluate performance against classical approaches

### Phase 3: Post-Quantum Security
- Assess post-quantum cryptography requirements
- Implement quantum-resistant authentication methods
- Prepare security infrastructure for quantum threats

### Phase 4: HPC Integration
- Establish HPC cluster connections
- Implement distributed computing patterns
- Scale quantum-inspired solutions

### Phase 5: Production Integration
- Bridge research implementations with production systems
- Deploy quantum-resistant security measures
- Scale quantum-inspired optimizations

## Implementation Approach

1. **Isolation First**
   - All quantum/HPC experiments run in containerized environments
   - Clear separation between research and production code
   - Modular design for easy integration

2. **Gradual Integration**
   - Start with quantum-inspired classical implementations
   - Introduce post-quantum security measures incrementally
   - Scale HPC capabilities based on demand

3. **Security Focus**
   - Prioritize quantum-resistant cryptography
   - Implement secure HPC access patterns
   - Maintain isolation of experimental features

## Technology Stack

- Quantum Simulation: Future integration with frameworks like Qiskit, Cirq
- Post-Quantum Cryptography: Planned integration with liboqs
- HPC Orchestration: Kubernetes-based scaling
- Development: TypeScript/Node.js with quantum computing libraries

## Success Metrics

- Performance improvements from quantum-inspired algorithms
- Security assessment of post-quantum implementations
- Scalability of HPC integrations
- Code quality and maintainability
- Integration readiness with production systems

## Risk Management

- Maintain classical fallbacks for all quantum-inspired features
- Regular security audits of post-quantum implementations
- Performance monitoring of HPC integrations
- Version control of all experimental code

## Next Steps

1. Begin quantum-inspired algorithm implementations
2. Evaluate post-quantum cryptography libraries
3. Establish HPC connection patterns
4. Document performance benchmarks
5. Plan gradual production integration

## References

- [Research Corner Documentation](../research-corner/README.md)
- [Quantum Experiments Service](../research-corner/quantum-experiments.service.ts)
- [Docker Configuration](../research-corner/Dockerfile.research)

This strategy will evolve as quantum computing technology matures and our requirements become more defined. Regular reviews and updates to this document will ensure our approach remains aligned with technological advances and business needs.

// File: research-corner\Dockerfile.research
----------------------------------------
# Stage 1: Development
FROM node:20-slim as development

# Install additional system dependencies that might be needed for quantum/HPC libraries
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    build-essential \
    && rm -rf /var/lib/apt/lists/*

WORKDIR /usr/src/app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm install

# Copy source files
COPY . .

# Build TypeScript code
RUN npm run build

# Stage 2: Production
FROM node:20-slim as production

# Install system dependencies needed for runtime
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    && rm -rf /var/lib/apt/lists/*

# Install potential future quantum/HPC Python dependencies
# Commented out for now, uncomment and modify as needed
# RUN pip3 install qiskit pennylane numpy scipy

WORKDIR /usr/src/app

# Copy package files
COPY package*.json ./

# Install production dependencies only
RUN npm install --only=production

# Copy built JavaScript files
COPY --from=development /usr/src/app/dist ./dist

# Set environment variables
ENV NODE_ENV=production
ENV POST_QUANTUM_EXPERIMENT=false
ENV QUANTUM_SIMULATOR_MODE=basic

# Create a non-root user
RUN useradd -r -u 1001 -g root quantum-user
USER quantum-user

# Start the service
CMD ["node", "dist/quantum-experiments.service.js"]

# Health check
HEALTHCHECK --interval=30s --timeout=3s \
    CMD curl -f http://localhost:3000/health || exit 1

# Expose port if needed
EXPOSE 3000

# Labels for documentation
LABEL maintainer="VAIM Team" \
      description="Research environment for quantum computing and HPC experiments" \
      version="0.1.0"

// File: research-corner\README.md
----------------------------------------
# Research Corner

This directory serves as a dedicated sandbox environment for exploring quantum computing and high-performance computing (HPC) capabilities within our system. It provides a structured foundation for experimenting with quantum-inspired algorithms, post-quantum cryptography, and advanced computational techniques without impacting our production services.

## Purpose

- Provide an isolated environment for quantum and HPC research and development
- Enable experimentation with quantum-inspired optimization algorithms
- Test integration of post-quantum cryptographic libraries
- Explore HPC orchestration and scaling patterns
- Serve as a proving ground for advanced computational concepts

## Structure

- `quantum-experiments.service.ts`: Core service for quantum-inspired experiments
- `Dockerfile.research`: Containerization setup for HPC/quantum environment
- Supporting configuration and utility files

## Future Integrations

This corner is designed to accommodate various quantum and HPC libraries such as:

- Post-quantum cryptography libraries (e.g., liboqs)
- Quantum-inspired optimization frameworks
- HPC orchestration tools
- Quantum simulation libraries

## Environment Configuration

The service supports the following environment variables:

```env
POST_QUANTUM_EXPERIMENT=true  # Enable post-quantum features
QUANTUM_SIMULATOR_MODE=basic  # Simulation complexity level
HPC_CLUSTER_ENDPOINT=        # Optional HPC cluster connection
```

## Integration Guidelines

While this corner is isolated from production code, it's designed to be easily integrated when needed:

1. Services can import quantum-inspired algorithms as modules
2. Post-quantum cryptography can be enabled via feature flags
3. HPC capabilities can be accessed through well-defined interfaces

## Development Workflow

1. Create a new experiment branch
2. Implement quantum-inspired or HPC features
3. Test in isolation using provided containers
4. Document findings and performance metrics
5. Optionally integrate with main services

## Getting Started

```bash
# Build the research environment
docker build -f Dockerfile.research -t quantum-research .

# Run the quantum experiments service
docker run -e POST_QUANTUM_EXPERIMENT=true quantum-research
```

## Security Considerations

- All quantum-inspired features are disabled by default
- Post-quantum cryptography experiments are isolated
- HPC resources are accessed through secure channels
- Separate containerization prevents production impact

## Contributing

When adding new experiments:

1. Document the theoretical background
2. Provide clear integration patterns
3. Include performance benchmarks
4. Consider production implications
5. Maintain isolation from core services

This research corner is part of our long-term strategy for quantum readiness and HPC integration. For more details, see our [quantum readiness documentation](../phases/quantum-readiness.md).

// File: tools\claude.md
----------------------------------------
# Claude 3.5 Sonnet Documentation

## Primary Role
- Primary Analytical Engine for the project

## Key Capabilities
1. **Complex Reasoning**
   - Performs high-level systematic analysis
   - Handles multi-step planning
2. **Document Analysis**
   - Analyzes large architectural documents
   - Formulates advanced bridging statements
3. **Concept Verification**
   - Verifies conceptual consistency in knowledge graph

## Usage Patterns

### Early Phase Decomposition
- Generates initial architectural overviews
- Creates high-level designs
- Provides system decomposition recommendations

### Methodological Verification
- Compares proposed solutions against design principles
- Validates architectural decisions
- Ensures system coherence

### Bridging Concept Generation
- Creates advanced bridging statements
- Identifies potential knowledge gaps
- Suggests new concept relationships

## Integration Points
- NLP Pipeline integration
- Graph Analytics validation
- AI-Driven Insight generation
- Documentation verification

// File: tools\docker.md
----------------------------------------
# Docker Setup Guide

## Overview
This document outlines the Docker configuration for the project's microservices architecture, with a focus on the authentication service.

## Prerequisites
- Docker Desktop installed and running
- Docker Compose installed
- Node.js and npm installed

## Development Environment

### Directory Structure
```
/services
  /auth-service
    Dockerfile
    docker-compose.yml
    docker-compose.test.yml
    .env
    .env.test
```

### Configuration Files

#### Dockerfile
```dockerfile
FROM node:16-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "run", "start:prod"]
```

#### Docker Compose (Development)
```yaml
version: '3.8'
services:
  auth-service:
    build: ./services/auth-service
    ports:
      - "3001:3000"
    depends_on:
      - auth-postgres
      - auth-neo4j
    environment:
      NODE_ENV: development
      DB_HOST: auth-postgres
      DB_PORT: 5432
      DB_USER: ${POSTGRES_USER:-admin}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-admin}
      DB_NAME: ${POSTGRES_DB:-auth}
      NEO4J_URI: bolt://auth-neo4j:7687
      NEO4J_USER: ${NEO4J_USER:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-password123}

  graph-analytics-service:
    build: ./services/graph-analytics-service
    ports:
      - "3002:3002"
    depends_on:
      - auth-neo4j
      - kafka
      - redis
    environment:
      NODE_ENV: development
      PORT: 3002
      NEO4J_URI: bolt://auth-neo4j:7687
      NEO4J_USERNAME: ${NEO4J_USER:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-password123}
      NEO4J_DATABASE: neo4j
      KAFKA_BROKERS: kafka:9092
      KAFKA_CLIENT_ID: graph-analytics-service
      KAFKA_GROUP_ID: graph-analytics-group
      REDIS_HOST: redis
      REDIS_PORT: 6379

  auth-postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-admin}
      POSTGRES_DB: ${POSTGRES_DB:-auth}
    ports:
      - "5433:5432"
    volumes:
      - auth-postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-admin} -d ${POSTGRES_DB:-auth}"]
      interval: 10s
      timeout: 5s
      retries: 5

  auth-neo4j:
    image: neo4j:5-enterprise
    environment:
      NEO4J_AUTH: ${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-password123}
      NEO4J_ACCEPT_LICENSE_AGREEMENT: 'yes'
      NEO4J_dbms_security_procedures_unrestricted: "gds.*"
      NEO4J_dbms_security_procedures_whitelist: "gds.*"
    ports:
      - "7475:7474"
      - "7688:7687"
    volumes:
      - auth-neo4j-data:/data
      - ./neo4j/plugins:/plugins
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9093:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    ports:
      - "2182:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-data:/data

volumes:
  auth-postgres-data:
  auth-neo4j-data:
  redis-data:

networks:
  auth-network:
    driver: bridge
  analytics-network:
    driver: bridge
```

#### Docker Compose (Testing)
```yaml
version: '3.8'
services:
  auth-service:
    build: ./services/auth-service
    ports:
      - "3001:3000"
    depends_on:
      - auth-postgres
      - auth-neo4j
    environment:
      NODE_ENV: test
      DB_HOST: auth-postgres
      DB_PORT: 5432
      DB_USER: ${POSTGRES_USER:-test}
      DB_PASSWORD: ${POSTGRES_PASSWORD:-test123}
      DB_NAME: ${POSTGRES_DB:-test_auth}
      NEO4J_URI: bolt://auth-neo4j:7687
      NEO4J_USER: ${NEO4J_USER:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-password123}

  graph-analytics-service:
    build: ./services/graph-analytics-service
    ports:
      - "3002:3002"
    depends_on:
      - auth-neo4j
      - kafka
      - redis
    environment:
      NODE_ENV: test
      PORT: 3002
      NEO4J_URI: bolt://auth-neo4j:7687
      NEO4J_USERNAME: ${NEO4J_USER:-neo4j}
      NEO4J_PASSWORD: ${NEO4J_PASSWORD:-password123}
      NEO4J_DATABASE: neo4j
      KAFKA_BROKERS: kafka:9092
      KAFKA_CLIENT_ID: graph-analytics-service-test
      KAFKA_GROUP_ID: graph-analytics-group-test
      REDIS_HOST: redis
      REDIS_PORT: 6379

  auth-postgres:
    image: postgres:15
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-test}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-test123}
      POSTGRES_DB: ${POSTGRES_DB:-test_auth}
    ports:
      - "5433:5432"
    volumes:
      - auth-postgres-test-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-test} -d ${POSTGRES_DB:-test_auth}"]
      interval: 10s
      timeout: 5s
      retries: 5

  auth-neo4j:
    image: neo4j:5-enterprise
    environment:
      NEO4J_AUTH: ${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:-password123}
      NEO4J_ACCEPT_LICENSE_AGREEMENT: 'yes'
      NEO4J_dbms_security_procedures_unrestricted: "gds.*"
      NEO4J_dbms_security_procedures_whitelist: "gds.*"
    ports:
      - "7475:7474"
      - "7688:7687"
    volumes:
      - auth-neo4j-test-data:/data
      - ./neo4j/plugins:/plugins
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - zookeeper
    ports:
      - "9093:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    ports:
      - "2182:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis-test-data:/data

volumes:
  auth-postgres-test-data:
  auth-neo4j-test-data:
  redis-test-data:

networks:
  auth-network:
    driver: bridge
  analytics-network:
    driver: bridge
```

## Usage

### Development Environment
1. Start the development environment:
   ```bash
   docker-compose up -d
   ```

2. Stop the development environment:
   ```bash
   docker-compose down
   ```

### Test Environment
1. Start the test environment:
   ```bash
   docker-compose -f docker-compose.test.yml up -d
   ```

2. Stop the test environment:
   ```bash
   docker-compose -f docker-compose.test.yml down
   ```

### Health Checks
- Auth Service: `curl http://localhost:3001/health`
- Graph Analytics Service: `curl http://localhost:3002/api/v1/analytics/health`
- PostgreSQL: `curl http://localhost:5433`
- Neo4j Browser: `http://localhost:7475`
- Neo4j Bolt: `bolt://localhost:7688`
- Kafka: `nc -zv localhost 9093`
- Zookeeper: `nc -zv localhost 2182`
- Redis: `nc -zv localhost 6380`

## Troubleshooting

### Common Issues
1. Port Conflicts
   - Solution: Update port mappings in docker-compose files
   - Example: Change "3001:3000" to "3002:3000"

2. Database Connection Issues
   - Check health check endpoints
   - Verify environment variables
   - Ensure services are healthy using `docker-compose ps`
   - For Neo4j: Ensure password meets minimum length requirement (8 characters)

3. Container Startup Order
   - Using `depends_on` with health checks
   - Proper service initialization order

### Port Mappings
Current service ports:
- Auth Service: 3001:3000
- Graph Analytics Service: 3002:3002
- PostgreSQL: 5433:5432 (changed to avoid conflicts)
- Neo4j Browser: 7475:7474 (changed to avoid conflicts)
- Neo4j Bolt: 7688:7687 (changed to avoid conflicts)
- Kafka: 9093:9092 (changed to avoid conflicts)
- Zookeeper: 2182:2181 (changed to avoid conflicts)
- Redis: 6380:6379 (changed to avoid conflicts)

### Service Configuration
Key environment variables:
- Neo4j: 
  - Default credentials: neo4j/password123 (minimum 8 characters required)
  - Enterprise edition with Graph Data Science library
- PostgreSQL:
  - Default credentials: admin/admin
- Redis:
  - No authentication required in development
- Kafka:
  - Bootstrap server: kafka:29092 (internal), localhost:9093 (external)

### Logs
View container logs:
```bash
docker-compose logs [service-name]
docker-compose -f docker-compose.test.yml logs [service-name]
```

## Best Practices
1. Use multi-stage builds for production
2. Implement proper health checks
3. Version control Docker files
4. Use environment variables for configuration
5. Regular security updates
6. Proper resource limits

## Related Documentation
- [CI/CD Pipeline](../workflows/cicd.md)
- [Phase 1 Timeline](../implementation/phase1-timeline.md)


// File: workflows\cicd.md
----------------------------------------
# CI/CD Pipeline Documentation

## Overview
This document outlines the Continuous Integration and Continuous Deployment (CI/CD) pipeline for the microservices architecture, focusing on the authentication service.

## Pipeline Structure

### Development Workflow
```mermaid
graph LR
    A[Developer Push] --> B[GitHub Actions]
    B --> C[Lint & Format]
    C --> D[Unit Tests]
    D --> E[Integration Tests]
    E --> F[Build Docker Image]
    F --> G[Push to Registry]
    G --> H[Deploy to Dev]
```

## GitHub Actions Configuration

### Main Workflow
```yaml
name: Auth Service CI/CD

on:
  push:
    branches: [ main ]
    paths:
      - 'services/auth-service/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'services/auth-service/**'

jobs:
  test:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: services/auth-service

    steps:
      - uses: actions/checkout@v2
      
      - name: Setup Node.js
        uses: actions/setup-node@v2
        with:
          node-version: '16'
          
      - name: Install Dependencies
        run: npm ci
        
      - name: Lint
        run: npm run lint
        
      - name: Run Tests
        run: npm test
        
      - name: Build
        run: npm run build

  docker:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v2
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v1
        
      - name: Login to Container Registry
        uses: docker/login-action@v1
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Build and Push
        uses: docker/build-push-action@v2
        with:
          context: services/auth-service
          push: true
          tags: ghcr.io/${{ github.repository }}/auth-service:latest
```

## Testing Strategy

### Unit Tests
- Run for every push and pull request
- Must pass before merge
- Coverage thresholds:
  ```javascript
  // jest.config.js
  coverageThreshold: {
    global: {
      branches: 80,
      functions: 80,
      lines: 80,
      statements: 80
    }
  }
  ```

### Integration Tests
- Run in isolated Docker environment
- Test database interactions
- API endpoint validation
- Health check verification

## Deployment Stages

### Development
- Automatic deployment on main branch
- Environment: Development cluster
- URL: dev.api.example.com

### Staging
- Manual trigger after development
- Environment: Staging cluster
- URL: staging.api.example.com

### Production
- Manual approval required
- Environment: Production cluster
- URL: api.example.com

## Environment Configuration

### Development
```yaml
environment:
  DB_HOST: postgres
  DB_PORT: 5432
  DB_USER: ${POSTGRES_USER}
  DB_PASSWORD: ${POSTGRES_PASSWORD}
  DB_NAME: auth
  NEO4J_URI: bolt://neo4j:7687
  NEO4J_USER: ${NEO4J_USER}
  NEO4J_PASSWORD: ${NEO4J_PASSWORD}
  NODE_ENV: development
```

### Production
```yaml
environment:
  DB_HOST: ${PROD_DB_HOST}
  DB_PORT: ${PROD_DB_PORT}
  DB_USER: ${PROD_DB_USER}
  DB_PASSWORD: ${PROD_DB_PASSWORD}
  DB_NAME: ${PROD_DB_NAME}
  NEO4J_URI: ${PROD_NEO4J_URI}
  NEO4J_USER: ${PROD_NEO4J_USER}
  NEO4J_PASSWORD: ${PROD_NEO4J_PASSWORD}
  NODE_ENV: production
```

## Monitoring & Alerts

### Health Checks
- Endpoint: `/health`
- Frequency: Every 30 seconds
- Alert on:
  - Response time > 1s
  - Status != 200
  - Database disconnection

### Metrics
- Request rate
- Error rate
- Response time
- Database connection pool
- Container resource usage

## Security Measures

### Secrets Management
- GitHub Secrets for credentials
- Environment-specific values
- Rotation policy: 90 days

### Container Security
- Regular base image updates
- Security scanning in pipeline
- No root user in containers

## Rollback Procedure

### Automatic Rollback
1. Monitor deployment health
2. Detect critical errors
3. Revert to last stable version
4. Alert development team

### Manual Rollback
```bash
# Revert to previous version
kubectl rollout undo deployment/auth-service

# Verify rollback
kubectl rollout status deployment/auth-service
```

## Related Documentation
- [Docker Setup](../tools/docker.md)
- [Phase 1 Timeline](../implementation/phase1-timeline.md)

// File: workflows\high-level-workflow.md
----------------------------------------
# High-Level Workflow Overview

## 1. User Ingestion & NLP
- Text ingestion from various sources
- Initial text processing by Claude 3.5 Sonnet
- NLP Microservice processing:
  - Morphological parsing
  - Concept extraction
- Neo4j graph updates

## 2. Graph Updates & Analytics
- Scheduled background jobs:
  - PageRank computation
  - Community detection
  - Centrality metrics
- Structural validation by O1 Pro

## 3. AI-Driven Insight Generation
- Subgraph analysis for bridging suggestions
- LLM-driven concept proposals
- User review and approval process

## 4. Collaborative Visualization
- Real-time graph visualization
- Interactive node/relationship management
- Multi-user collaboration features

## 5. Continuous Evolution
- Microservice maintenance
- CI/CD pipeline management
- Architectural validation
- Documentation updates

// File: services\auth-service\.env.example
----------------------------------------
# Database Configuration
# Note: For production, use a secrets management service like HashiCorp Vault, AWS Secrets Manager, or GCP Secret Manager
# These example values are for local development only
DB_HOST=auth-postgres
DB_PORT=5433  # Changed from 5432 to avoid conflicts
DB_USER=admin
DB_PASSWORD=admin  # In production, use a strong password stored in a secrets manager
DB_NAME=auth

# Neo4j Configuration
# Note: For production, use a secrets management service
NEO4J_URI=bolt://auth-neo4j:7688  # Changed from 7687 to avoid conflicts
NEO4J_USER=neo4j
NEO4J_PASSWORD=password123  # Minimum 8 characters required, use a strong password in production

# Application Configuration
PORT=3000
NODE_ENV=development  # Use 'production' in production environment

# JWT Configuration (Add in production)
JWT_SECRET=your-secret-key  # Store this in a secrets manager for production
JWT_EXPIRATION=24h

# Redis Configuration (Add in production)
# Note: For production, use a secrets management service
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=your-redis-password  # Use a strong password in production

# DeepSeek AI Configuration
# DEEPSEEK_API_KEY=  # Get key from https://platform.deepseek.com/api-keys

# SSL/TLS Configuration (Add in production)
# SSL_KEY_PATH=/path/to/ssl/key
# SSL_CERT_PATH=/path/to/ssl/certificate

# Rate Limiting (Add in production)
# RATE_LIMIT_WINDOW=15m
# RATE_LIMIT_MAX_REQUESTS=100

# Frontend Configuration
FRONTEND_URL=http://localhost:4200  # URL of the frontend application

# OAuth2 Configuration
OAUTH2_STATE_SECRET=your-state-secret  # Secret for OAuth2 state parameter
OAUTH2_ENABLED_PROVIDERS=google,github  # Comma-separated list of enabled providers

# Google OAuth2
GOOGLE_CLIENT_ID=your-google-client-id
GOOGLE_CLIENT_SECRET=your-google-client-secret
GOOGLE_CALLBACK_URL=http://localhost:3000/auth/oauth2/google/callback

# GitHub OAuth2
GITHUB_CLIENT_ID=your-github-client-id
GITHUB_CLIENT_SECRET=your-github-client-secret
GITHUB_CALLBACK_URL=http://localhost:3000/auth/oauth2/github/callback


// File: services\auth-service\.env.test
----------------------------------------
# Test Database Configuration
DB_HOST=localhost
DB_PORT=5432
DB_USER=test
DB_PASSWORD=test
DB_NAME=test_auth

# Test Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=test

# Application Configuration
PORT=3001
NODE_ENV=test

// File: services\auth-service\.eslintrc.js
----------------------------------------
module.exports = {
  parser: '@typescript-eslint/parser',
  parserOptions: {
    project: 'tsconfig.json',
    sourceType: 'module',
  },
  plugins: ['@typescript-eslint/eslint-plugin'],
  extends: [
    'plugin:@typescript-eslint/recommended',
    'plugin:prettier/recommended',
    'plugin:nestjs/recommended',
  ],
  root: true,
  env: {
    node: true,
    jest: true,
  },
  ignorePatterns: ['.eslintrc.js'],
  rules: {
    '@typescript-eslint/interface-name-prefix': 'off',
    '@typescript-eslint/explicit-function-return-type': 'off',
    '@typescript-eslint/explicit-module-boundary-types': 'off',
    '@typescript-eslint/no-explicit-any': 'off',
    'prettier/prettier': [
      'error',
      {
        endOfLine: 'auto',
      },
    ],
  },
};

// File: services\auth-service\.gitignore
----------------------------------------
# Dependencies
node_modules/

# Environment files
.env
.env.local

# Build artifacts
dist/
build/

# Logs
logs/
*.log

# Editor directories and files
.idea/
.vscode/
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?

# Test coverage
coverage/
.nyc_output/

# Misc
.DS_Store
Thumbs.db

// File: services\auth-service\.prettierrc
----------------------------------------
{
  "printWidth": 120,
  "tabWidth": 2,
  "useTabs": false,
  "semi": true,
  "singleQuote": true,
  "trailingComma": "all",
  "bracketSpacing": true,
  "arrowParens": "always"
}

// File: services\auth-service\docker-compose.test.yml
----------------------------------------
version: '3.8'

services:
  auth-postgres-test:
    image: postgres:13-alpine
    environment:
      POSTGRES_USER: test
      POSTGRES_PASSWORD: test
      POSTGRES_DB: test_auth
    ports:
      - '5432:5432'
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U test']
      interval: 5s
      timeout: 5s
      retries: 5
    volumes:
      - auth-postgres-test-data:/var/lib/postgresql/data
    networks:
      - auth-test-network

  auth-neo4j-test:
    image: neo4j:4.4
    environment:
      NEO4J_AUTH: neo4j/test
      NEO4J_ACCEPT_LICENSE_AGREEMENT: 'yes'
    ports:
      - '7474:7474'
      - '7687:7687'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    volumes:
      - auth-neo4j-test-data:/data
    networks:
      - auth-test-network

  auth-service:
    build: .
    environment:
      DB_HOST: auth-postgres-test
      DB_PORT: 5432
      DB_USER: test
      DB_PASSWORD: test
      DB_NAME: test_auth
      NEO4J_URI: bolt://auth-neo4j-test:7687
      NEO4J_USER: neo4j
      NEO4J_PASSWORD: test
      NODE_ENV: test
    depends_on:
      auth-postgres-test:
        condition: service_healthy
      auth-neo4j-test:
        condition: service_healthy
    ports:
      - '3002:3001'
    networks:
      - auth-test-network

volumes:
  auth-postgres-test-data:
  auth-neo4j-test-data:

networks:
  auth-test-network:
    driver: bridge

// File: services\auth-service\docker-compose.yml
----------------------------------------
version: '3.8'

services:
  postgres:
    image: postgres:latest
    environment:
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
      POSTGRES_DB: ${DB_NAME}
    ports:
      - '5432:5432'
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:alpine
    ports:
      - '6379:6379'
    command: redis-server --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data

  auth-service:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - '3000:3000'
    environment:
      - DB_HOST=postgres
      - DB_PORT=${DB_PORT}
      - DB_USER=${DB_USER}
      - DB_PASSWORD=${DB_PASSWORD}
      - DB_NAME=${DB_NAME}
      - JWT_SECRET=${JWT_SECRET}
      - JWT_EXPIRATION=${JWT_EXPIRATION}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=${REDIS_PASSWORD}
    depends_on:
      - postgres
      - redis

volumes:
  postgres_data:
  redis_data:

// File: services\auth-service\Dockerfile
----------------------------------------
# Use official Node.js image
FROM node:18

# Create app directory
WORKDIR /usr/src/app

# Install app dependencies
COPY package*.json ./
RUN npm install

# Bundle app source
COPY . .

# Expose port
EXPOSE 3000

# Run the app
CMD ["npm", "start"]

// File: services\auth-service\jest.config.js
----------------------------------------
module.exports = {
  moduleFileExtensions: ['js', 'json', 'ts'],
  rootDir: 'src',
  testRegex: '.*\\.spec\\.ts$',
  transform: {
    '^.+\\.(t|j)s$': 'ts-jest',
  },
  collectCoverageFrom: ['**/*.(t|j)s'],
  coverageDirectory: '../coverage',
  testEnvironment: 'node',
  moduleNameMapper: {
    '^@/(.*)$': '<rootDir>/$1',
  },
};

// File: services\auth-service\README.md
----------------------------------------
# Auth Service

Authentication and authorization service for the application.

## Features
- User authentication
- Role-based access control
- JWT token management

## Setup

1. Clone the repository
2. Install dependencies:
   ```bash
   npm install
   ```
3. Copy `.env.example` to `.env` and configure environment variables
4. Start the service:
   ```bash
   npm run start:dev
   ```

## Docker Setup

The service uses Docker Compose for local development with the following containers:
- `auth-service`: The main authentication service
- `auth-postgres`: PostgreSQL database
- `auth-neo4j`: Neo4j graph database

Container naming follows the pattern `{service}-{database}` to avoid conflicts when running multiple microservices.

### Production Considerations

#### Secrets Management
For production deployments:
- DO NOT use the environment variables in docker-compose.yml directly
- Use a secrets management service like:
  - HashiCorp Vault
  - AWS Secrets Manager
  - GCP Secret Manager
  - Azure Key Vault
- Store sensitive data including:
  - Database credentials
  - JWT secrets
  - API keys
  - SSL/TLS certificates

#### Security Best Practices
1. Use non-root users in containers
2. Enable SSL/TLS for all services
3. Implement rate limiting
4. Use strong passwords and rotate them regularly
5. Enable database encryption at rest
6. Regular security audits and updates

## Environment Variables

| Variable         | Description                     | Default           |
|------------------|---------------------------------|-------------------|
| DB_HOST          | PostgreSQL host                 | auth-postgres     |
| DB_PORT          | PostgreSQL port                 | 5432              |
| DB_USER          | PostgreSQL user                 | admin             |
| DB_PASSWORD      | PostgreSQL password             | admin             |
| DB_NAME          | PostgreSQL database name        | auth              |
| NEO4J_URI        | Neo4j connection URI            | bolt://auth-neo4j:7687 |
| NEO4J_USER       | Neo4j username                  | neo4j             |
| NEO4J_PASSWORD   | Neo4j password                  | admin             |
| PORT             | Service port                    | 3000              |
| NODE_ENV         | Node environment                | development       |

**Note:** Default values are for local development only. Use secure values in production.

## API Documentation

### Health Check
- **GET** `/health`
  - Returns service health status including database connectivity

## Development

- Start development server:
  ```bash
  npm run start:dev
  ```

- Run tests:
  ```bash
  npm test
  ```

- Lint code:
  ```bash
  npm run lint
  ```

- Build for production:
  ```bash
  npm run build
  ```

## Production Deployment

1. Set up secrets management service
2. Configure environment-specific variables
3. Enable SSL/TLS
4. Set up monitoring and logging
5. Configure backup strategy
6. Implement CI/CD pipeline with security checks

For detailed deployment instructions, see [deployment guide](docs/deployment.md).

// File: services\graph-analytics-service\.env.example
----------------------------------------
# Service Configuration
PORT=3002
NODE_ENV=development

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=password
NEO4J_DATABASE=neo4j

# Kafka Configuration
KAFKA_BROKERS=localhost:9092
KAFKA_CLIENT_ID=graph-analytics-service
KAFKA_GROUP_ID=graph-analytics-group
KAFKA_SSL_ENABLED=false

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# Analytics Configuration
ANALYTICS_BATCH_SIZE=1000
ANALYTICS_SCHEDULE_ENABLED=true
ANALYTICS_SCHEDULE_CRON=0 0 * * *

# Security Configuration
JWT_SECRET=your-jwt-secret
JWT_EXPIRATION=3600

# Logging Configuration
LOG_LEVEL=debug
ENABLE_REQUEST_LOGGING=true

# Monitoring Configuration
ENABLE_METRICS=true
METRICS_PORT=9464

# Data Retention Configuration
DATA_RETENTION_DAYS=90
DATA_RETENTION_ARCHIVE_PATH=/data/archives
DATA_RETENTION_SCHEDULE=0 0 3 * * * # Run at 3 AM daily

# Pipeline Configuration
PIPELINE_DAILY_ANALYTICS_ENABLED=true
PIPELINE_WEEKLY_ANALYTICS_ENABLED=true
PIPELINE_DAILY_ANALYTICS_TIME=0 0 0 * * * # Run at midnight
PIPELINE_WEEKLY_ANALYTICS_TIME=0 0 0 * * 0 # Run at midnight on Sundays


// File: services\graph-analytics-service\.env.test
----------------------------------------
# Service Configuration
PORT=3002
NODE_ENV=test

# Neo4j Configuration
NEO4J_URI=bolt://test-neo4j:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=test
NEO4J_DATABASE=neo4j

# Kafka Configuration
KAFKA_BROKERS=test-kafka:29092
KAFKA_CLIENT_ID=graph-analytics-service-test
KAFKA_GROUP_ID=graph-analytics-group-test
KAFKA_SSL_ENABLED=false

# Redis Configuration
REDIS_HOST=test-redis
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# Analytics Configuration
ANALYTICS_BATCH_SIZE=100
ANALYTICS_SCHEDULE_ENABLED=false

# Security
JWT_SECRET=test-jwt-secret
JWT_EXPIRATION=3600

# Logging
LOG_LEVEL=debug
ENABLE_REQUEST_LOGGING=true

# Monitoring
ENABLE_METRICS=false
METRICS_PORT=9464

// File: services\graph-analytics-service\.eslintrc.js
----------------------------------------
module.exports = {
  parser: '@typescript-eslint/parser',
  parserOptions: {
    project: 'tsconfig.json',
    tsconfigRootDir: __dirname,
    sourceType: 'module',
  },
  plugins: ['@typescript-eslint/eslint-plugin'],
  extends: [
    'plugin:@typescript-eslint/recommended',
    'plugin:prettier/recommended',
  ],
  root: true,
  env: {
    node: true,
    jest: true,
  },
  ignorePatterns: ['.eslintrc.js'],
  rules: {
    '@typescript-eslint/interface-name-prefix': 'off',
    '@typescript-eslint/explicit-function-return-type': 'off',
    '@typescript-eslint/explicit-module-boundary-types': 'off',
    '@typescript-eslint/no-explicit-any': 'off',
    '@typescript-eslint/no-unused-vars': ['warn', { argsIgnorePattern: '^_' }],
    'prettier/prettier': [
      'error',
      {
        endOfLine: 'auto',
      },
    ],
  },
};

// File: services\graph-analytics-service\.gitignore
----------------------------------------
# compiled output
/dist
/node_modules

# Logs
logs
*.log
npm-debug.log*
pnpm-debug.log*
yarn-debug.log*
yarn-error.log*
lerna-debug.log*

# OS
.DS_Store

# Tests
/coverage
/.nyc_output

# IDEs and editors
/.idea
.project
.classpath
.c9/
*.launch
.settings/
*.sublime-workspace

# IDE - VSCode
.vscode/*
!.vscode/settings.json
!.vscode/tasks.json
!.vscode/launch.json
!.vscode/extensions.json

# Environment files
.env
.env.local
.env.*.local

# Neo4j data
/neo4j/data/

# Redis data
/redis/data/

# Kafka data
/kafka/data/

# Docker volumes
/docker/volumes/

# Temporary files
*.swp
*.swo
*~

# Build artifacts
*.tsbuildinfo

// File: services\graph-analytics-service\.prettierrc
----------------------------------------
{
  "singleQuote": true,
  "trailingComma": "all",
  "printWidth": 100,
  "tabWidth": 2,
  "semi": true,
  "bracketSpacing": true,
  "arrowParens": "avoid",
  "endOfLine": "auto"
}

// File: services\graph-analytics-service\docker-compose.test.yml
----------------------------------------
version: '3.8'

services:
  test-neo4j:
    image: neo4j:5-enterprise
    environment:
      NEO4J_AUTH: neo4j/test
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      NEO4J_dbms_security_procedures_unrestricted: "gds.*"
      NEO4J_dbms_security_procedures_whitelist: "gds.*"
    ports:
      - "7475:7474"  # Different port to avoid conflicts with dev environment
      - "7688:7687"
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:7474 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - test-network

  test-kafka:
    image: confluentinc/cp-kafka:7.4.0
    depends_on:
      - test-zookeeper
    ports:
      - "9093:9092"  # Different port to avoid conflicts with dev environment
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: test-zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://test-kafka:29092,PLAINTEXT_HOST://localhost:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    networks:
      - test-network

  test-zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    ports:
      - "2182:2181"  # Different port to avoid conflicts with dev environment
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - test-network

  test-redis:
    image: redis:7-alpine
    ports:
      - "6380:6379"  # Different port to avoid conflicts with dev environment
    command: redis-server --appendonly yes
    networks:
      - test-network

  graph-analytics-service-test:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      - test-neo4j
      - test-kafka
      - test-redis
    environment:
      NODE_ENV: test
      PORT: 3002
      NEO4J_URI: bolt://test-neo4j:7687
      NEO4J_USERNAME: neo4j
      NEO4J_PASSWORD: test
      NEO4J_DATABASE: neo4j
      KAFKA_BROKERS: test-kafka:29092
      KAFKA_CLIENT_ID: graph-analytics-service-test
      KAFKA_GROUP_ID: graph-analytics-group-test
      REDIS_HOST: test-redis
      REDIS_PORT: 6379
      JWT_SECRET: test-jwt-secret
    command: npm run test:e2e
    networks:
      - test-network

networks:
  test-network:
    driver: bridge

// File: services\graph-analytics-service\Dockerfile
----------------------------------------
# Build stage
FROM node:18-alpine AS builder

WORKDIR /usr/src/app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci

# Copy source code
COPY . .

# Build application
RUN npm run build

# Production stage
FROM node:18-alpine

# Install additional system dependencies for Neo4j and Kafka clients
RUN apk add --no-cache \
    libc6-compat \
    openssl \
    python3 \
    make \
    g++

WORKDIR /usr/src/app

# Copy package files
COPY package*.json ./

# Install production dependencies only
RUN npm ci --only=production

# Copy built application from builder stage
COPY --from=builder /usr/src/app/dist ./dist

# Copy necessary configuration files
COPY --from=builder /usr/src/app/.env* ./

# Create non-root user
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nestjs -u 1001 -G nodejs

# Set ownership
RUN chown -R nestjs:nodejs /usr/src/app

# Switch to non-root user
USER nestjs

# Expose service port
EXPOSE 3002

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=30s \
  CMD wget --no-verbose --tries=1 --spider http://localhost:3002/health || exit 1

# Start application
CMD ["node", "dist/main"]

// File: services\graph-analytics-service\jest.config.js
----------------------------------------
module.exports = {
  moduleFileExtensions: ['js', 'json', 'ts'],
  rootDir: '.',
  testRegex: '.*\\.spec\\.ts$',
  transform: {
    '^.+\\.(t|j)s$': 'ts-jest',
  },
  collectCoverageFrom: ['**/*.(t|j)s'],
  coverageDirectory: './coverage',
  testEnvironment: 'node',
  roots: ['<rootDir>/src/'],
  moduleNameMapper: {
    '^@app/(.*)$': '<rootDir>/src/$1',
    '^@config/(.*)$': '<rootDir>/src/config/$1',
    '^@analytics/(.*)$': '<rootDir>/src/analytics/$1',
    '^@integration/(.*)$': '<rootDir>/src/integration/$1',
    '^@common/(.*)$': '<rootDir>/src/common/$1',
  },
  setupFilesAfterEnv: ['<rootDir>/test/setup.ts'],
  coverageThreshold: {
    global: {
      branches: 70,
      functions: 70,
      lines: 70,
      statements: 70,
    },
  },
  testTimeout: 30000,
};

// File: services\graph-analytics-service\README.md
----------------------------------------
# Graph Analytics Service

A microservice for performing advanced graph analytics using Neo4j Graph Data Science (GDS) library and providing data integration capabilities.

## Features

- Graph Analytics
  - PageRank computation
  - Community detection
  - Node similarity analysis
  - Shortest path finding
- Graph Management
  - Graph projections creation and management
  - Real-time analytics processing
- Integration Support
  - Neo4j GDS integration
  - Event streaming with Kafka
  - Caching with Redis

## Prerequisites

- Node.js (v18 or later)
- Neo4j Database (with Graph Data Science library installed)
- Apache Kafka (optional, for event streaming)
- Redis (optional, for caching)

## Installation

```bash
# Install dependencies
npm install

# Build the service
npm run build
```

## Configuration

Create a `.env` file in the root directory with the following variables:

```env
# Service Configuration
PORT=3002
NODE_ENV=development

# Neo4j Configuration
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your-password
NEO4J_DATABASE=neo4j

# Kafka Configuration (Optional)
KAFKA_BROKERS=localhost:9092
KAFKA_CLIENT_ID=graph-analytics-service
KAFKA_GROUP_ID=graph-analytics-group
KAFKA_SSL_ENABLED=false

# Redis Configuration (Optional)
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0

# Analytics Configuration
ANALYTICS_BATCH_SIZE=1000
ANALYTICS_SCHEDULE_ENABLED=true
ANALYTICS_SCHEDULE_CRON="0 0 * * *"

# Security
JWT_SECRET=your-jwt-secret
JWT_EXPIRATION=3600
```

## Running the Service

```bash
# Development
npm run start:dev

# Production
npm run start:prod
```

## API Documentation

Once the service is running, visit `http://localhost:3002/api/docs` to access the Swagger documentation.

### Available Endpoints

- `POST /api/v1/analytics/pagerank` - Compute PageRank for nodes
- `POST /api/v1/analytics/communities` - Detect communities in the graph
- `POST /api/v1/analytics/similarity` - Calculate node similarity
- `POST /api/v1/analytics/shortest-path` - Find shortest path between nodes
- `POST /api/v1/analytics/graph-projections` - Create a graph projection
- `DELETE /api/v1/analytics/graph-projections/:name` - Drop a graph projection
- `GET /api/v1/analytics/health` - Check Neo4j connection health

## Testing

```bash
# Unit tests
npm run test

# E2E tests
npm run test:e2e

# Test coverage
npm run test:cov
```

## Docker Support

Build and run the service using Docker:

```bash
# Build the image
docker build -t graph-analytics-service .

# Run the container
docker run -p 3002:3002 --env-file .env graph-analytics-service
```

## Contributing

1. Create a feature branch
2. Commit your changes
3. Push to the branch
4. Create a Pull Request

## License

This project is proprietary and confidential.

// File: services\llm-service\.env.e2e
----------------------------------------
# Server Configuration
PORT=3003
NODE_ENV=test
API_PREFIX=api/v1
FRONTEND_URL=http://localhost:3000

# Redis Configuration
REDIS_HOST=redis-test
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=2
REDIS_CACHE_TTL=1

# Cache Configuration
CACHE_ENABLED=false

# OpenRouter Configuration (Mock values for e2e testing)
OPENROUTER_API_KEY=test-key
SITE_URL=http://localhost:3000
SITE_NAME=VAIM2 E2E Test
OPENROUTER_MAX_RETRIES=1
OPENROUTER_TIMEOUT=1000

# Provider Configuration
DEFAULT_LLM_PROVIDER=openrouter
DEFAULT_MODEL=deepseek/deepseek-r1

# Optional Direct Provider Keys (Mock values for e2e testing)
DEEPSEEK_API_KEY=test-key
DEEPSEEK_MODEL=deepseek-r1
DEEPSEEK_MAX_RETRIES=1
DEEPSEEK_TIMEOUT=1000

# Rate Limiting
RATE_LIMIT_TTL=1
RATE_LIMIT_MAX=1000

# Performance
MAX_CONCURRENT_REQUESTS=50
REQUEST_TIMEOUT=1000
BATCH_SIZE=10

# Cost Management
DAILY_COST_LIMIT=100
ALERT_THRESHOLD=90

# Monitoring
METRICS_PORT=9466
LOG_LEVEL=error
LOG_FORMAT=json

# WebSocket
WS_PORT=3004

# GraphQL
GRAPHQL_PLAYGROUND=false
GRAPHQL_DEBUG=false
GRAPHQL_INTROSPECTION=true

# Security
JWT_SECRET=test-jwt-secret
JWT_EXPIRATION=1m
REFRESH_TOKEN_SECRET=test-refresh-token-secret
REFRESH_TOKEN_EXPIRATION=5m

# CORS
CORS_ENABLED=true
CORS_CREDENTIALS=true

# Test Configuration
TEST_TIMEOUT=30000
RETRY_ATTEMPTS=3
RETRY_DELAY=1000

// File: services\llm-service\.env.example
----------------------------------------
# Server Configuration
PORT=3002
NODE_ENV=development
API_PREFIX=api/v1

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_CACHE_TTL=3600

# GraphQL Configuration
GRAPHQL_DEBUG=true
GRAPHQL_PLAYGROUND=true
GRAPHQL_INTROSPECTION=true

# LLM Provider Configuration
DEFAULT_LLM_PROVIDER=claude # claude, gpt4, deepseek, etc.

# Claude Configuration
CLAUDE_API_KEY=
CLAUDE_MODEL=claude-3-sonnet
CLAUDE_MAX_TOKENS=4096
CLAUDE_TEMPERATURE=0.7

# OpenAI Configuration (Fallback)
OPENAI_API_KEY=
OPENAI_MODEL=gpt-4
OPENAI_MAX_TOKENS=4096
OPENAI_TEMPERATURE=0.7

# Rate Limiting
RATE_LIMIT_TTL=60
RATE_LIMIT_MAX=100

# Security
JWT_SECRET=your-jwt-secret
JWT_EXPIRATION=15m

# Neo4j Integration
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=

# Monitoring
PROMETHEUS_ENABLED=true
METRICS_PORT=9464

# Logging
LOG_LEVEL=debug
LOG_FORMAT=json

# WebSocket
WS_PORT=3003

# Cache Configuration
CACHE_ENABLED=true
CACHE_TTL=3600
MAX_CACHE_SIZE=1000

# Cost Management
COST_TRACKING_ENABLED=true
DAILY_COST_LIMIT=50
ALERT_THRESHOLD=45

# Performance
MAX_CONCURRENT_REQUESTS=50
REQUEST_TIMEOUT=30000
BATCH_SIZE=10

// File: services\llm-service\.env.test
----------------------------------------
# Server Configuration
PORT=3002
NODE_ENV=test
API_PREFIX=api/v1
FRONTEND_URL=http://localhost:3000

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=1
REDIS_CACHE_TTL=60

# Cache Configuration
CACHE_ENABLED=true

# OpenRouter Configuration
OPENROUTER_API_KEY=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
SITE_URL=http://localhost:3000
SITE_NAME=VAIM2 Test
OPENROUTER_MAX_RETRIES=1
OPENROUTER_TIMEOUT=1000

# Provider Configuration
DEFAULT_LLM_PROVIDER=openrouter
DEFAULT_MODEL=deepseek/deepseek-r1

# Optional Direct Provider Keys (Mock values for testing)
DEEPSEEK_API_KEY=test-key
DEEPSEEK_MODEL=deepseek-r1
DEEPSEEK_MAX_RETRIES=1
DEEPSEEK_TIMEOUT=1000

# Rate Limiting
RATE_LIMIT_TTL=1
RATE_LIMIT_MAX=100

# Performance
MAX_CONCURRENT_REQUESTS=10
REQUEST_TIMEOUT=1000
BATCH_SIZE=5

# Cost Management
DAILY_COST_LIMIT=10
ALERT_THRESHOLD=9

# Monitoring
METRICS_PORT=9465
LOG_LEVEL=error
LOG_FORMAT=json

# WebSocket
WS_PORT=3004

# GraphQL
GRAPHQL_PLAYGROUND=false
GRAPHQL_DEBUG=false
GRAPHQL_INTROSPECTION=false

# Security
JWT_SECRET=test-jwt-secret
JWT_EXPIRATION=1m
REFRESH_TOKEN_SECRET=test-refresh-token-secret
REFRESH_TOKEN_EXPIRATION=5m

# CORS
CORS_ENABLED=true
CORS_CREDENTIALS=true

// File: services\llm-service\docker-compose.test.yml
----------------------------------------
version: '3.8'

services:
  llm-service-test:
    build:
      context: .
      target: builder
    container_name: llm-service-test
    command: npm run test
    environment:
      - NODE_ENV=test
      - REDIS_HOST=redis-test
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - REDIS_DB=1
      - OPENROUTER_API_KEY=test-key
      - SITE_URL=http://localhost:3000
      - SITE_NAME=VAIM2 Test
      - OPENROUTER_BASE_URL=http://mock-openrouter:3000/api/v1
    volumes:
      - .:/app
      - /app/node_modules
    depends_on:
      redis-test:
        condition: service_healthy
      mock-openrouter:
        condition: service_started
    networks:
      - llm-test-network

  llm-service-e2e:
    build:
      context: .
      target: builder
    container_name: llm-service-e2e
    command: npm run test:e2e
    environment:
      - NODE_ENV=test
      - REDIS_HOST=redis-test
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - REDIS_DB=2
      - OPENROUTER_API_KEY=test-key
      - SITE_URL=http://localhost:3000
      - SITE_NAME=VAIM2 E2E
      - OPENROUTER_BASE_URL=http://mock-openrouter:3000/api/v1
    volumes:
      - .:/app
      - /app/node_modules
    depends_on:
      redis-test:
        condition: service_healthy
      mock-openrouter:
        condition: service_started
    networks:
      - llm-test-network

  mock-openrouter:
    build:
      context: ./test/mock-openrouter
    container_name: mock-openrouter
    ports:
      - "3001:3000"
    networks:
      - llm-test-network
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/v1/models"]
      interval: 1s
      timeout: 3s
      retries: 30

  redis-test:
    image: redis:7-alpine
    container_name: llm-redis-test
    command: redis-server --appendonly no --save ""
    ports:
      - "6380:6379"
    networks:
      - llm-test-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 30

networks:
  llm-test-network:
    driver: bridge

// File: services\llm-service\docker-compose.yml
----------------------------------------
version: '3.8'

services:
  llm-service:
    build:
      context: .
      target: development
    container_name: llm-service
    ports:
      - "3002:3002"  # API
      - "9464:9464"  # Metrics
      - "3003:3003"  # WebSocket
    volumes:
      - .:/app
      - /app/node_modules
    environment:
      - NODE_ENV=development
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_PASSWORD=
      - REDIS_DB=0
    depends_on:
      - redis
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3002/health"]
      interval: 30s
      timeout: 3s
      retries: 3
      start_period: 30s

  redis:
    image: redis:7-alpine
    container_name: llm-redis
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    networks:
      - llm-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s

  redis-commander:
    image: rediscommander/redis-commander:latest
    container_name: llm-redis-commander
    ports:
      - "8081:8081"
    environment:
      - REDIS_HOSTS=local:redis:6379
    depends_on:
      - redis
    networks:
      - llm-network

networks:
  llm-network:
    driver: bridge

volumes:
  redis-data:
    driver: local

// File: services\llm-service\Dockerfile
----------------------------------------
# Build stage
FROM node:18-alpine AS builder

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install dependencies
RUN npm ci

# Copy source code
COPY . .

# Build application
RUN npm run build

# Production stage
FROM node:18-alpine AS production

WORKDIR /app

# Copy package files
COPY package*.json ./

# Install production dependencies only
RUN npm ci --only=production

# Copy built application from builder stage
COPY --from=builder /app/dist ./dist
COPY --from=builder /app/node_modules ./node_modules

# Copy necessary files
COPY .env.example .env

# Create non-root user
RUN addgroup -g 1001 -S nodejs && \
    adduser -S nestjs -u 1001

# Set ownership
RUN chown -R nestjs:nodejs /app

# Switch to non-root user
USER nestjs

# Expose ports
EXPOSE 3002
EXPOSE 9464

# Health check
HEALTHCHECK --interval=30s --timeout=3s --start-period=30s \
  CMD wget --no-verbose --tries=1 --spider http://localhost:3002/health || exit 1

# Start application
CMD ["node", "dist/main"]

// File: services\llm-service\env.example
----------------------------------------
# Server Configuration
PORT=3002
NODE_ENV=development
API_PREFIX=api/v1
FRONTEND_URL=http://localhost:3000

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_CACHE_TTL=3600

# Cache Configuration
CACHE_ENABLED=true

# OpenRouter Configuration
OPENROUTER_API_KEY=your-openrouter-api-key
SITE_URL=http://localhost:3000
SITE_NAME=VAIM2 Platform
OPENROUTER_MAX_RETRIES=3
OPENROUTER_TIMEOUT=30000

# Provider Configuration
DEFAULT_LLM_PROVIDER=openrouter
DEFAULT_MODEL=deepseek/deepseek-r1

# Optional Direct Provider Keys
DEEPSEEK_API_KEY=
DEEPSEEK_MODEL=deepseek-r1
DEEPSEEK_MAX_RETRIES=3
DEEPSEEK_TIMEOUT=30000

# Rate Limiting
RATE_LIMIT_TTL=60
RATE_LIMIT_MAX=100

# Performance
MAX_CONCURRENT_REQUESTS=50
REQUEST_TIMEOUT=30000
BATCH_SIZE=10

# Cost Management
DAILY_COST_LIMIT=50
ALERT_THRESHOLD=45

# Monitoring
METRICS_PORT=9464
LOG_LEVEL=debug
LOG_FORMAT=json

# WebSocket
WS_PORT=3003

# GraphQL
GRAPHQL_PLAYGROUND=true
GRAPHQL_DEBUG=true
GRAPHQL_INTROSPECTION=true

# Security
JWT_SECRET=your-jwt-secret
JWT_EXPIRATION=15m
REFRESH_TOKEN_SECRET=your-refresh-token-secret
REFRESH_TOKEN_EXPIRATION=7d

# CORS
CORS_ENABLED=true
CORS_CREDENTIALS=true

# Documentation
API_TITLE=VAIM2 LLM Service
API_DESCRIPTION=LLM integration service for the VAIM2 platform
API_VERSION=1.0.0
API_PATH=/api/docs

// File: services\llm-service\README.md
----------------------------------------
# LLM Service

The LLM (Large Language Model) service provides a unified interface for accessing various LLM providers through both GraphQL and REST APIs. It supports multiple providers including OpenRouter for accessing a wide range of models, and direct provider integrations for specific use cases.

## Features

- Multiple LLM provider support
- GraphQL and REST APIs
- Real-time streaming responses
- Response caching
- Rate limiting
- Health monitoring
- Provider fallback
- Cost tracking

## Prerequisites

- Node.js 18+
- Redis 6+
- OpenRouter API key
- (Optional) Direct provider API keys

## Installation

```bash
# Install dependencies
npm install

# Copy environment file
cp .env.example .env

# Update environment variables
# Edit .env with your configuration
```

## Configuration

### Environment Variables

```env
# Server Configuration
PORT=3002
NODE_ENV=development
API_PREFIX=api/v1

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_CACHE_TTL=3600

# OpenRouter Configuration
OPENROUTER_API_KEY=your-api-key
SITE_URL=your-site-url
SITE_NAME=your-site-name

# Provider Configuration
DEFAULT_LLM_PROVIDER=openrouter
DEFAULT_MODEL=deepseek/deepseek-r1

# Optional Direct Provider Keys
DEEPSEEK_API_KEY=
CLAUDE_API_KEY=

# Performance
MAX_CONCURRENT_REQUESTS=50
REQUEST_TIMEOUT=30000
BATCH_SIZE=10
```

## Usage

### Starting the Service

```bash
# Development
npm run start:dev

# Production
npm run build
npm run start:prod
```

### API Examples

#### GraphQL

```graphql
# Text Completion
query {
  complete(
    messages: [
      { role: "user", content: "What is the meaning of life?" }
    ],
    options: {
      model: "deepseek/deepseek-r1",
      temperature: 0.7
    }
  ) {
    text
    usage {
      promptTokens
      completionTokens
      totalTokens
    }
    metadata {
      model
      provider
      latency
      timestamp
    }
  }
}

# Streaming Completion
subscription {
  streamCompletion(streamId: "unique-id") {
    text
    metadata {
      model
      provider
      latency
      timestamp
    }
  }
}
```

#### REST

```bash
# Text Completion
curl -X POST http://localhost:3002/api/v1/llm/complete \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      { "role": "user", "content": "What is the meaning of life?" }
    ],
    "options": {
      "model": "deepseek/deepseek-r1",
      "temperature": 0.7
    }
  }'

# Streaming Completion
curl -N http://localhost:3002/api/v1/llm/complete/stream \
  -H "Accept: text/event-stream" \
  -G \
  --data-urlencode 'messages=[{"role":"user","content":"What is the meaning of life?"}]'
```

## Development

### Running Tests

```bash
# Unit tests
npm run test

# E2E tests
npm run test:e2e

# Test coverage
npm run test:cov
```

### Docker

```bash
# Build image
docker build -t llm-service .

# Run container
docker run -p 3002:3002 llm-service
```

### Docker Compose

```bash
# Start services
docker-compose up -d

# Stop services
docker-compose down
```

## Health Checks

The service provides health check endpoints:

- `/health` - Overall service health
- `/health/live` - Liveness check
- `/health/ready` - Readiness check

## Monitoring

The service exposes metrics for Prometheus:

- Request counts and latencies
- Cache hit rates
- Token usage
- Provider availability
- Error rates

## Documentation

- [Architecture](docs/architecture.md)
- [API Documentation](docs/api.md)
- [Provider Integration](docs/providers.md)
- [Configuration Guide](docs/configuration.md)

## Contributing

1. Fork the repository
2. Create your feature branch
3. Commit your changes
4. Push to the branch
5. Create a new Pull Request

## License

This project is proprietary and confidential.

// File: services\auth-service\docs\implementation.md
----------------------------------------
# Auth Service Implementation

## Overview
The authentication service provides secure user authentication and authorization through multiple methods:
- Local authentication (username/password)
- OAuth2 authentication (Google, GitHub)
- JWT-based token management
- Secure refresh token rotation

## Components

### Core Authentication
- JWT-based token generation and validation
- Secure password hashing with bcrypt
- Redis-based token blacklisting
- Refresh token rotation for enhanced security

### OAuth2 Integration
- Multi-provider support (Google, GitHub)
- Provider-specific strategies with profile mapping
- Secure state parameter validation
- PKCE support for enhanced security
- Token storage and management in Redis
- Automatic user creation/linking for OAuth2 users

### Security Features
- Role-based access control (RBAC)
- Token blacklisting for secure logout
- Refresh token rotation
- Rate limiting
- CSRF protection
- Secure session management

## Configuration

### Environment Variables
```bash
# JWT Configuration
JWT_SECRET=your-secret-key
JWT_EXPIRATION=15m
REFRESH_TOKEN_EXPIRATION=7d

# Redis Configuration
REDIS_HOST=redis
REDIS_PORT=6379
REDIS_PASSWORD=your-redis-password

# Frontend Configuration
FRONTEND_URL=http://localhost:4200

# OAuth2 Configuration
OAUTH2_STATE_SECRET=your-state-secret
OAUTH2_ENABLED_PROVIDERS=google,github

# Google OAuth2
GOOGLE_CLIENT_ID=your-google-client-id
GOOGLE_CLIENT_SECRET=your-google-client-secret
GOOGLE_CALLBACK_URL=http://localhost:3000/auth/oauth2/google/callback

# GitHub OAuth2
GITHUB_CLIENT_ID=your-github-client-id
GITHUB_CLIENT_SECRET=your-github-client-secret
GITHUB_CALLBACK_URL=http://localhost:3000/auth/oauth2/github/callback
```

## API Endpoints

### Local Authentication
- `POST /auth/login` - Login with email/password
- `POST /auth/refresh` - Refresh access token
- `POST /auth/logout` - Logout and invalidate tokens

### OAuth2 Authentication
- `GET /auth/oauth2/:provider` - Initiate OAuth2 flow
- `GET /auth/oauth2/:provider/callback` - Handle OAuth2 callback

## Token Management

### Access Tokens
- Short-lived JWT tokens (15 minutes)
- Contains user ID, email, and roles
- Used for API authentication

### Refresh Tokens
- Long-lived tokens (7 days)
- Stored in Redis with user association
- One-time use with automatic rotation
- Can be revoked server-side

## Security Considerations

### OAuth2 Security
- State parameter validation
- PKCE implementation
- Secure token storage
- Provider-specific security measures
- Automatic user linking
- Profile data validation

### General Security
- Password hashing with bcrypt
- Token encryption
- HTTPS-only cookies
- CSRF protection
- Rate limiting
- Input validation
- Secure headers

## Testing

### Unit Tests
- Authentication flows
- Token management
- Password hashing
- Input validation

### Integration Tests
- OAuth2 provider integration
- Token refresh flow
- User management
- Error handling

### E2E Tests
- Complete authentication flows
- OAuth2 provider flows
- Error scenarios
- Security measures

## Monitoring

### Metrics
- Authentication success/failure rates
- Token refresh rates
- OAuth2 provider usage
- Error rates by type
- Response times

### Alerts
- High failure rates
- Unusual traffic patterns
- Token validation failures
- Redis connectivity issues

## Maintenance

### Regular Tasks
- Token cleanup
- Session management
- Security updates
- Provider API updates
- Performance monitoring

### Backup Procedures
- Redis data backup
- Configuration backup
- User data backup
- Audit log backup

// File: services\auth-service\docs\oauth2-implementation.md
----------------------------------------
# OAuth2 Implementation Guide

## Overview
This document details the OAuth2 integration plan for the authentication service, outlining the implementation steps, configuration requirements, and security considerations.

## Implementation Steps

### 1. Provider Configuration
- Configure multiple OAuth2 providers (e.g., Google, GitHub, Microsoft)
- Set up provider-specific client credentials
- Define callback URLs for each provider
- Implement provider-specific user profile mapping

### 2. OAuth2 Strategy Implementation
```typescript
// Example OAuth2 Strategy Structure
@Injectable()
export class OAuth2Strategy extends PassportStrategy(Strategy, 'oauth2') {
  constructor(
    private configService: ConfigService,
    private authService: AuthService,
  ) {
    super({
      authorizationURL: 'provider-specific-url',
      tokenURL: 'provider-specific-token-url',
      clientID: configService.get('OAUTH2_CLIENT_ID'),
      clientSecret: configService.get('OAUTH2_CLIENT_SECRET'),
      callbackURL: configService.get('OAUTH2_CALLBACK_URL'),
      scope: ['required-scopes'],
    });
  }

  async validate(accessToken: string, refreshToken: string, profile: any) {
    // Map provider profile to our user model
    // Create or update user
    // Return user entity
  }
}
```

### 3. Controller Endpoints
```typescript
// Required endpoints
@Controller('auth/oauth2')
export class OAuth2Controller {
  @Get(':provider')
  @UseGuards(OAuth2AuthGuard)
  async auth(@Param('provider') provider: string) {
    // Initiate OAuth2 flow
  }

  @Get(':provider/callback')
  @UseGuards(OAuth2AuthGuard)
  async callback(@Param('provider') provider: string) {
    // Handle OAuth2 callback
    // Generate JWT tokens
    // Return tokens to client
  }
}
```

### 4. User Profile Mapping
- Define provider-specific profile interfaces
- Implement profile mapping logic
- Handle user creation/linking
- Manage provider-specific user metadata

## Security Considerations

### Token Security
- Secure storage of provider tokens
- Token encryption at rest
- Refresh token rotation
- Token scope limitations

### Provider-Specific Security
- Validate state parameter
- Implement PKCE (Proof Key for Code Exchange)
- Verify token signatures
- Handle provider-specific security requirements

### Error Handling
- Invalid state parameter
- Token validation failures
- Provider API errors
- User cancellation handling

## Configuration Requirements

### Environment Variables
```bash
# General OAuth2 Configuration
OAUTH2_ENABLED_PROVIDERS=google,github
OAUTH2_STATE_SECRET=your-state-secret

# Google OAuth2
GOOGLE_CLIENT_ID=your-google-client-id
GOOGLE_CLIENT_SECRET=your-google-client-secret
GOOGLE_CALLBACK_URL=http://localhost:3000/auth/oauth2/google/callback

# GitHub OAuth2
GITHUB_CLIENT_ID=your-github-client-id
GITHUB_CLIENT_SECRET=your-github-client-secret
GITHUB_CALLBACK_URL=http://localhost:3000/auth/oauth2/github/callback
```

### Required Dependencies
```json
{
  "dependencies": {
    "passport-google-oauth20": "^2.0.0",
    "passport-github2": "^0.1.12",
    "passport-oauth2": "^1.7.0"
  }
}
```

## Testing Strategy

### Unit Tests
- Test provider strategy validation
- Test profile mapping logic
- Test token generation/validation

### Integration Tests
- Test OAuth2 flow initiation
- Test callback handling
- Test user creation/linking
- Test error scenarios

### E2E Tests
- Complete OAuth2 flow testing
- Multiple provider testing
- Error handling scenarios
- Token management verification

## Implementation Timeline

1. **Week 1: Basic Setup**
   - Install required dependencies
   - Configure environment variables
   - Set up basic strategy structure

2. **Week 2: Provider Integration**
   - Implement Google OAuth2
   - Implement GitHub OAuth2
   - Test basic flows

3. **Week 3: User Management**
   - Implement profile mapping
   - Handle user creation/linking
   - Add provider-specific metadata

4. **Week 4: Testing & Security**
   - Implement security measures
   - Write comprehensive tests
   - Document API endpoints

## Monitoring & Maintenance

### Metrics to Track
- OAuth2 flow success rate
- Provider-specific error rates
- Token refresh success rate
- User linking statistics

### Maintenance Tasks
- Regular token cleanup
- Provider API version monitoring
- Security update management
- Configuration review

## API Documentation

### OAuth2 Flow Endpoints

#### Initiate OAuth2 Flow
```
GET /auth/oauth2/:provider
```
- Initiates the OAuth2 authentication flow
- Redirects to provider login page
- Supports multiple providers via path parameter

#### OAuth2 Callback
```
GET /auth/oauth2/:provider/callback
```
- Handles provider callback
- Validates state and tokens
- Returns JWT access/refresh tokens

## Error Handling

### Common Error Scenarios
1. Invalid state parameter
2. Token validation failure
3. User cancellation
4. Provider API errors
5. Profile mapping errors

### Error Responses
```typescript
{
  statusCode: number;
  message: string;
  error: string;
  details?: any;
}

// File: services\graph-analytics-service\docs\data-pipeline.md
----------------------------------------
# Data Pipeline Implementation

## Overview
The data pipeline implementation provides automated analytics processing, data retention management, and monitoring capabilities for the graph analytics service.

## Components

### Pipeline Service
- Manages scheduled analytics jobs
- Handles data retention policies
- Integrates with monitoring system
- Configurable through environment variables

### Monitoring Service
- Prometheus metrics integration
- Tracks job durations and failures
- Monitors data retention operations
- Exposes metrics endpoint for collection

### Data Retention
- Configurable retention period (default: 90 days)
- Automated cleanup of old data
- Data archival capabilities
- Transaction-safe deletion process

## Configuration

### Environment Variables

```env
# Monitoring Configuration
ENABLE_METRICS=true
METRICS_PORT=9464

# Data Retention Configuration
DATA_RETENTION_DAYS=90
DATA_RETENTION_ARCHIVE_PATH=/data/archives
DATA_RETENTION_SCHEDULE=0 0 3 * * * # Run at 3 AM daily

# Pipeline Configuration
PIPELINE_DAILY_ANALYTICS_ENABLED=true
PIPELINE_WEEKLY_ANALYTICS_ENABLED=true
PIPELINE_DAILY_ANALYTICS_TIME=0 0 0 * * * # Run at midnight
PIPELINE_WEEKLY_ANALYTICS_TIME=0 0 0 * * 0 # Run at midnight on Sundays
```

## Scheduled Jobs

### Daily Analytics
- PageRank computation
- Community detection
- Data retention enforcement
- Runs at configured time (default: midnight)

### Weekly Analytics
- Node similarity analysis
- Graph statistics computation
- Runs at configured time (default: midnight on Sundays)

## Monitoring Metrics

### Job Metrics
- `graph_analytics_job_duration_seconds`: Histogram of analytics job duration
- `graph_analytics_job_failures_total`: Counter of failed analytics jobs

### Data Retention Metrics
- `graph_data_retention_deletions_total`: Counter of deleted nodes/relationships

## Error Handling
- Failed jobs are logged and tracked in Prometheus
- Automatic retry mechanism for failed operations
- Alert configuration available through Prometheus

## Best Practices
1. Monitor the metrics endpoint for job performance
2. Adjust retention period based on data growth
3. Configure archive path with sufficient storage
4. Review job schedules to minimize impact on system performance

## Security Considerations
- Metrics endpoint requires authentication
- Archived data is encrypted
- Access to retention operations is restricted
- Audit logging of all retention activities

// File: services\llm-service\docs\api.md
----------------------------------------
# LLM Service API Documentation

## Overview

The LLM Service provides a unified interface for interacting with various language model providers, with OpenRouter as the primary integration. The service supports both REST and GraphQL endpoints, with features including streaming responses, caching, and comprehensive error handling.

## REST Endpoints

### POST /llm/complete

Generates a completion for the given messages.

#### Request

```json
{
  "messages": [
    {
      "role": "user",
      "content": "Your message here"
    }
  ],
  "model": "deepseek/deepseek-r1", // Optional, defaults to configured default model
  "temperature": 0.7, // Optional, defaults to 0.7
  "maxTokens": 4096, // Optional, defaults to 4096
  "topP": 1.0, // Optional, defaults to 1.0
  "frequencyPenalty": 0.0, // Optional
  "presencePenalty": 0.0, // Optional
  "stop": [] // Optional, array of stop sequences
}
```

#### Response

```json
{
  "text": "Generated response text",
  "usage": {
    "promptTokens": 10,
    "completionTokens": 20,
    "totalTokens": 30
  },
  "metadata": {
    "provider": "openrouter",
    "model": "deepseek/deepseek-r1",
    "latency": 1234,
    "timestamp": "2025-01-21T18:30:00.000Z",
    "cached": false
  }
}
```

### POST /llm/complete/stream

Streams the completion response as it's generated.

#### Request
Same as /llm/complete

#### Response
Server-Sent Events stream with the following event format:

```json
{
  "text": "Partial response text",
  "metadata": {
    "provider": "openrouter",
    "model": "deepseek/deepseek-r1",
    "latency": 1234,
    "timestamp": "2025-01-21T18:30:00.000Z"
  }
}
```

### GET /llm/health

Returns the health status of the service and its dependencies.

#### Response

```json
{
  "status": "ok",
  "providers": {
    "openrouter": true,
    "openrouterOpenAI": true
  },
  "redis": true
}
```

## GraphQL API

### Queries

#### complete

```graphql
query {
  complete(input: {
    messages: [{ role: String!, content: String! }!]!
    model: String
    temperature: Float
    maxTokens: Int
    topP: Float
    frequencyPenalty: Float
    presencePenalty: Float
    stop: [String!]
  }) {
    text
    usage {
      promptTokens
      completionTokens
      totalTokens
    }
    metadata {
      provider
      model
      latency
      timestamp
      cached
    }
  }
}
```

### Subscriptions

#### completionStream

```graphql
subscription {
  completionStream(input: {
    messages: [{ role: String!, content: String! }!]!
    model: String
    temperature: Float
    maxTokens: Int
    topP: Float
    frequencyPenalty: Float
    presencePenalty: Float
    stop: [String!]
  }) {
    text
    metadata {
      provider
      model
      latency
      timestamp
    }
  }
}
```

## Error Handling

The service returns standardized error responses with the following structure:

```json
{
  "statusCode": 400,
  "message": "Error message",
  "error": "Error type"
}
```

### Error Types

- `PROVIDER_ERROR`: Issues with the LLM provider (e.g., invalid API key)
- `RATE_LIMIT`: Rate limit exceeded
- `CONTEXT_LENGTH`: Input exceeds model's maximum context length
- `TIMEOUT`: Request timed out
- `MODEL_NOT_FOUND`: Requested model not available
- `INVALID_REQUEST`: Invalid request parameters
- `UNKNOWN`: Unexpected errors

### HTTP Status Codes

- 200: Successful request
- 201: Successful creation/completion
- 400: Invalid request
- 401: Authentication error
- 429: Rate limit exceeded
- 500: Server error

## Caching

The service implements Redis-based caching for completion requests:

- Cache key: Generated from the request parameters (messages, model, temperature, etc.)
- TTL: Configurable via REDIS_CACHE_TTL environment variable
- Cache bypass: Set CACHE_ENABLED=false in environment variables

Cached responses include `metadata.cached: true` to indicate a cache hit.

// File: services\llm-service\docs\architecture.md
----------------------------------------
# LLM Service Architecture

## Overview

The LLM Service is designed with a modular architecture that separates concerns and enables easy extension with new providers. The service integrates with OpenRouter as the primary LLM provider, with support for both direct API calls and OpenAI SDK integration.

## Core Components

```mermaid
graph TD
    A[API Layer] --> B[LLM Service]
    B --> C[Provider Factory]
    C --> D[OpenRouter Provider]
    C --> E[OpenRouter OpenAI Provider]
    B --> F[Redis Cache]
    A --> G[GraphQL Resolvers]
    G --> B
```

### API Layer
- REST endpoints for completions and health checks
- GraphQL resolvers for queries and subscriptions
- Request validation and error handling
- Response formatting

### LLM Service
- Core business logic
- Provider selection and management
- Caching strategy
- Error handling and retries
- Request/response transformation

### Provider Factory
- Provider instantiation and configuration
- Provider health checks
- Model mapping and validation
- Fallback handling

### Redis Cache
- Response caching
- Cache invalidation
- TTL management
- Health monitoring

## Test Infrastructure

```mermaid
graph TD
    A[Test Runner] --> B[Unit Tests]
    A --> C[Integration Tests]
    A --> D[E2E Tests]
    C --> E[Mock OpenRouter API]
    C --> F[Redis Test Instance]
    D --> E
    D --> F
    E --> G[Rate Limiter]
    E --> H[Error Simulator]
    E --> I[Stream Handler]
```

### Mock OpenRouter API
- Simulates OpenRouter API responses
- Implements rate limiting
- Handles streaming responses
- Simulates various error conditions
- Configurable latency and timeouts

### Test Containers
- Redis test instance
- Mock OpenRouter API service
- Isolated test network
- Health checks and readiness probes

### Test Coverage
- Unit tests: Provider implementations, service logic
- Integration tests: Redis caching, provider integration
- E2E tests: API endpoints, GraphQL resolvers

## Data Flow

```mermaid
sequenceDiagram
    participant Client
    participant API
    participant Cache
    participant Service
    participant Provider
    
    Client->>API: Request completion
    API->>Cache: Check cache
    alt Cache hit
        Cache-->>API: Return cached response
        API-->>Client: Return response
    else Cache miss
        Cache-->>Service: No cache entry
        Service->>Provider: Request completion
        Provider-->>Service: Return completion
        Service->>Cache: Store response
        Service-->>API: Return response
        API-->>Client: Return response
    end
```

## Error Handling

```mermaid
graph TD
    A[Error Occurs] --> B{Error Type}
    B -->|Provider Error| C[Retry with Backoff]
    B -->|Rate Limit| D[Queue Request]
    B -->|Context Length| E[Return Error]
    B -->|Timeout| F[Retry with Timeout]
    C --> G{Max Retries?}
    G -->|Yes| H[Return Error]
    G -->|No| I[Retry Request]
```

### Error Types
- Provider errors: Authentication, availability
- Rate limiting: Per-provider and global limits
- Context length: Model-specific limits
- Timeouts: Network and provider timeouts
- Validation: Request parameter validation

## Caching Strategy

```mermaid
graph TD
    A[Request] --> B{Cache Enabled?}
    B -->|Yes| C{Cache Entry Exists?}
    B -->|No| D[Forward Request]
    C -->|Yes| E[Return Cached]
    C -->|No| F[Forward Request]
    F --> G[Cache Response]
    G --> H[Return Response]
```

### Cache Implementation
- Redis as primary cache store
- Configurable TTL per response
- Cache key generation based on request parameters
- Cache invalidation on error
- Cache bypass for streaming requests

## Provider Implementation

```mermaid
graph TD
    A[Provider Factory] --> B[Base Provider Interface]
    B --> C[OpenRouter Direct]
    B --> D[OpenRouter OpenAI]
    C --> E[Axios Client]
    D --> F[OpenAI SDK]
    E --> G[Error Handling]
    F --> G
    G --> H[Response Mapping]
```

### Provider Interface
- Standard methods for all providers
- Error handling and retries
- Response transformation
- Health checks
- Configuration validation

## Testing Architecture

```mermaid
graph TD
    A[Test Suite] --> B[Unit Tests]
    A --> C[Integration Tests]
    A --> D[E2E Tests]
    B --> E[Jest]
    C --> F[Docker Compose]
    D --> F
    F --> G[Mock API]
    F --> H[Redis]
    G --> I[Rate Limiter]
    G --> J[Error Simulator]
```

### Test Components
- Mock OpenRouter API for integration testing
- Redis instance for cache testing
- Docker Compose for test environment
- Jest for test execution
- Supertest for HTTP testing

## Monitoring and Health Checks

```mermaid
graph TD
    A[Health Check] --> B[Provider Status]
    A --> C[Redis Status]
    A --> D[API Status]
    B --> E[Aggregate Status]
    C --> E
    D --> E
    E --> F[Health Endpoint]
```

### Health Monitoring
- Provider availability checks
- Redis connection status
- API endpoint health
- Response time monitoring
- Error rate tracking

// File: services\llm-service\docs\configuration.md
----------------------------------------
# LLM Service Configuration Guide

## Environment Variables

### Core Configuration
```env
NODE_ENV=development|test|production
PORT=3000
```

### OpenRouter Configuration
```env
OPENROUTER_API_KEY=your-api-key
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1
SITE_URL=http://your-site.com
SITE_NAME=Your Site Name
DEFAULT_MODEL=deepseek/deepseek-r1
```

### Redis Configuration
```env
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_PASSWORD=
REDIS_DB=0
REDIS_CACHE_TTL=3600
CACHE_ENABLED=true
```

### Provider Settings
```env
DEFAULT_LLM_PROVIDER=openrouter
MAX_RETRIES=3
REQUEST_TIMEOUT=30000
```

## Test Environment Configuration

### Docker Compose Test Setup

The test environment uses Docker Compose to run integration tests with mock services:

```yaml
services:
  # Test service container
  llm-service-test:
    build:
      context: .
      target: builder
    environment:
      - NODE_ENV=test
      - REDIS_HOST=redis-test
      - OPENROUTER_BASE_URL=http://mock-openrouter:3000/api/v1
      - OPENROUTER_API_KEY=test-key

  # Mock OpenRouter API
  mock-openrouter:
    build:
      context: ./test/mock-openrouter
    ports:
      - "3001:3000"

  # Redis test instance
  redis-test:
    image: redis:7-alpine
    command: redis-server --appendonly no --save ""
```

### Mock OpenRouter API

The test environment includes a mock OpenRouter API that simulates:
- Success responses
- Rate limiting (10 requests per minute)
- Context length errors (>8192 tokens)
- Timeouts
- Authentication errors
- Streaming responses

### Test Environment Variables (.env.test)
```env
NODE_ENV=test
REDIS_HOST=redis-test
REDIS_PORT=6379
REDIS_DB=1
OPENROUTER_API_KEY=test-key
SITE_URL=http://localhost:3000
SITE_NAME=VAIM2 Test
OPENROUTER_BASE_URL=http://mock-openrouter:3000/api/v1
```

## Provider Configuration

### OpenRouter Direct Provider

The OpenRouter direct provider uses axios to communicate with the OpenRouter API:

```typescript
{
  apiKey: string;
  defaultModel?: string;
  baseUrl?: string;
  siteUrl?: string;
  siteName?: string;
  maxRetries?: number;
  timeout?: number;
}
```

### OpenRouter OpenAI Provider

The OpenRouter OpenAI provider uses the OpenAI SDK configured for OpenRouter:

```typescript
{
  apiKey: string;
  defaultModel?: string;
  baseUrl?: string;
  siteUrl?: string;
  siteName?: string;
  maxRetries?: number;
  timeout?: number;
}
```

## Caching Configuration

### Redis Cache Settings
- TTL: Configurable via `REDIS_CACHE_TTL` (default: 3600 seconds)
- Cache key format: `llm:{hash of request parameters}`
- Cache bypass: Set `CACHE_ENABLED=false`

### Cache Key Generation
Cache keys are generated based on:
- Messages content and role
- Model selection
- Temperature
- Max tokens
- Top P
- Frequency penalty
- Presence penalty
- Stop sequences

## Testing

### Running Tests
```bash
# Unit tests
npm run test

# Integration tests
npm run test:integration

# E2E tests
npm run test:e2e
```

### Test Coverage Requirements
- Unit test coverage: >90%
- Integration test coverage: >80%
- E2E test coverage: >70%

### Performance Metrics
- Response time: <200ms (p95)
- Cache hit rate: >80%
- Error rate: <1%

## Error Handling

### Retry Strategy
- Maximum retries: Configurable via `MAX_RETRIES`
- Exponential backoff: `Math.pow(2, retryCount) * 1000ms`
- Retryable errors:
  - Network timeouts
  - Rate limiting
  - 5xx server errors

### Error Types
- `PROVIDER_ERROR`: Provider-specific errors
- `RATE_LIMIT`: Rate limit exceeded
- `CONTEXT_LENGTH`: Maximum context length exceeded
- `TIMEOUT`: Request timeout
- `MODEL_NOT_FOUND`: Model not available
- `INVALID_REQUEST`: Invalid parameters
- `UNKNOWN`: Unexpected errors

## Monitoring

### Health Checks
- Provider health: API accessibility
- Redis health: Connection status
- Overall service health: Combined status

### Metrics
- Request latency
- Cache hit rate
- Error rate
- Token usage
- Provider availability

// File: services\llm-service\docs\implementation.md
----------------------------------------
# LLM Service Implementation Status

## Current Status

The LLM service has been set up with the following components:

### Core Infrastructure
- ✅ Project structure and configuration
- ✅ Docker containerization
- ✅ Environment configuration
- ✅ Testing infrastructure
- ✅ Documentation

### API Layer
- ✅ REST API setup
- ✅ GraphQL API setup
- ✅ WebSocket support
- ✅ Health monitoring endpoints

### Provider Integration
- ✅ Provider interface definition
- ✅ Provider factory implementation
- ✅ OpenRouter integration
- ✅ DeepSeek direct integration
- ✅ Error handling

### Caching & Performance
- ✅ Redis integration
- ✅ Response caching
- ✅ Rate limiting
- ✅ Performance monitoring

### Testing
- ✅ Unit test setup
- ✅ Integration test setup
- ✅ E2E test setup
- ✅ Test documentation

## Next Steps

### 1. Cache Implementation (Priority: High)
- [ ] Implement Redis service
  - [ ] Connection management
  - [ ] Cache operations
  - [ ] Error handling
- [ ] Cache strategies
  - [ ] Response caching
  - [ ] Rate limit tracking
  - [ ] Token usage tracking

### 3. API Implementation (Priority: High)
- [ ] REST endpoints
  - [ ] Completion endpoint
  - [ ] Streaming endpoint
  - [ ] Provider management
- [ ] GraphQL resolvers
  - [ ] Query resolvers
  - [ ] Subscription resolvers
  - [ ] Type definitions

### 4. Monitoring Implementation (Priority: Medium)
- [ ] Health checks
  - [ ] Provider health
  - [ ] Redis health
  - [ ] System health
- [ ] Metrics collection
  - [ ] Request metrics
  - [ ] Cache metrics
  - [ ] Provider metrics

### 5. Security Implementation (Priority: High)
- [ ] Rate limiting
  - [ ] Global limits
  - [ ] Per-user limits
  - [ ] Provider limits
- [ ] Input validation
  - [ ] Request validation
  - [ ] Token limits
  - [ ] Content filtering

### 6. Testing Implementation (Priority: Medium)
- [ ] Unit tests
  - [ ] Provider tests
  - [ ] Service tests
  - [ ] Controller tests
- [ ] Integration tests
  - [ ] API tests
  - [ ] Cache tests
  - [ ] Provider tests
- [ ] E2E tests
  - [ ] Complete flow tests
  - [ ] Error scenarios
  - [ ] Performance tests

## Implementation Details

### Provider Implementation

#### OpenRouter Integration
The OpenRouter integration has been completed with two implementations:

1. Direct API Implementation (`OpenRouterProvider`)
```typescript
// Features implemented:
- API client with proper headers and configuration
- Full request/response mapping
- Comprehensive error handling with retries
  - Context length errors
  - Rate limiting
  - Authentication errors
  - Network timeouts
- Streaming support with proper event handling
  - SSE parsing and processing
  - Chunk aggregation
  - Error recovery
- Rate limiting and timeout handling
- Unit tests with mocked responses
  - Error scenarios
  - Streaming responses
  - Edge cases
```

2. OpenAI-Compatible Implementation (`OpenRouterOpenAIProvider`)
```typescript
// Features implemented:
- OpenAI SDK integration
- Streaming support via OpenAI interface
  - Chunk processing
  - Token counting
  - Metadata handling
- Proper error mapping from OpenAI to LLM errors
  - Context length detection
  - API errors
  - Network issues
- Configurable timeout and retry logic
- Full test coverage with mocked SDK
  - Error handling
  - Streaming
  - Response mapping
```

Both implementations support:
- Multiple model selection
- Custom request parameters
- Usage tracking
- Health monitoring
- Proper error propagation

#### DeepSeek Integration
```typescript
// Implementation steps:
1. Direct API integration
2. Model management
3. Token counting
4. Response processing
5. Error mapping
```

### Cache Implementation

#### Redis Service
```typescript
// Implementation steps:
1. Connection pool
2. Cache operations
3. Error handling
4. Health checks
```

#### Cache Strategies
```typescript
// Implementation steps:
1. Response caching
2. Cache invalidation
3. Cache size management
4. Performance optimization
```

### API Implementation

#### REST Controllers
```typescript
// Implementation steps:
1. Route handlers
2. Request validation
3. Response formatting
4. Error handling
```

#### GraphQL Resolvers
```typescript
// Implementation steps:
1. Query resolvers
2. Mutation resolvers
3. Subscription setup
4. Type definitions
```

## Testing Strategy

### Unit Testing
- Test individual components in isolation
- Mock external dependencies
- Focus on business logic
- Ensure high coverage

### Integration Testing
- Test component interactions
- Use test containers
- Verify API contracts
- Test error scenarios

### E2E Testing
- Test complete flows
- Verify real provider integration
- Test performance
- Validate monitoring

## Deployment Strategy

### Development
1. Local development setup
2. Docker Compose environment
3. Test environment
4. Documentation

### Staging
1. Provider integration testing
2. Performance testing
3. Security testing
4. Monitoring setup

### Production
1. Production configuration
2. Scaling setup
3. Monitoring
4. Backup strategy

## Maintenance Plan

### Regular Tasks
1. Log rotation
2. Cache cleanup
3. Metric collection
4. Health monitoring

### Updates
1. Provider API updates
2. Security patches
3. Dependency updates
4. Performance tuning

## Success Criteria

1. Provider Integration
- [ ] Successful API integration
- [ ] Error handling
- [ ] Performance metrics
- [ ] Documentation

2. Cache Performance
- [ ] Hit rate > 80%
- [ ] Response time < 100ms
- [ ] Memory usage < 1GB
- [ ] Error rate < 0.1%

3. API Reliability
- [ ] Uptime > 99.9%
- [ ] Response time < 200ms
- [ ] Error rate < 1%
- [ ] Rate limit compliance

4. Monitoring
- [ ] Real-time metrics
- [ ] Alert system
- [ ] Performance tracking
- [ ] Cost monitoring

// File: services\llm-service\docs\next-steps.md
----------------------------------------
# LLM Service - Next Implementation Phase

## Context
- Project: VAIM2 Phase 4 Implementation
- Current Branch: master (commit b4ee04f)
- Service: llm-service
- Documentation: /services/llm-service/docs/*

## Task Objective
Complete the OpenRouter provider integration with comprehensive testing and documentation, focusing on production readiness and maintainability.

## Implementation Requirements

### 1. Integration Testing (Priority: High)
- [ ] Test Container Setup
  - [ ] Docker Compose test configuration
  - [ ] Redis test container
  - [ ] Mock OpenRouter API container
  - [ ] Test environment variables

- [ ] Provider Integration Tests
  - [ ] OpenRouter direct provider
    - [ ] Success scenarios
    - [ ] Error handling
    - [ ] Rate limiting
    - [ ] Timeout handling
  - [ ] OpenRouter OpenAI provider
    - [ ] SDK integration
    - [ ] Error mapping
    - [ ] Configuration validation

### 2. E2E Testing (Priority: High)
- [ ] API Endpoint Tests
  - [ ] REST endpoints
    - [ ] Completion endpoint
    - [ ] Streaming endpoint
    - [ ] Health checks
  - [ ] GraphQL resolvers
    - [ ] Query resolvers
    - [ ] Subscription resolvers
    - [ ] Error handling

- [ ] Test Scenarios
  - [ ] Authentication
  - [ ] Rate limiting
  - [ ] Caching
  - [ ] Load testing
  - [ ] Error scenarios

### 3. Documentation Updates (Priority: Medium)
- [ ] API Documentation
  - [ ] OpenAPI/Swagger specs
  - [ ] GraphQL schema documentation
  - [ ] Response formats
  - [ ] Error codes

- [ ] Provider Documentation
  - [ ] Configuration guide
  - [ ] Environment variables
  - [ ] Rate limits
  - [ ] Error handling

- [ ] Architecture Documentation
  - [ ] Component diagrams
  - [ ] Sequence diagrams
  - [ ] Data flow diagrams
  - [ ] Caching strategy

### 4. Performance Optimization (Priority: Medium)
- [ ] Caching Implementation
  - [ ] Response caching
  - [ ] Token usage tracking
  - [ ] Cache invalidation
  - [ ] Memory management

- [ ] Rate Limiting
  - [ ] Provider-specific limits
  - [ ] User quotas
  - [ ] Burst handling

## Reference Files
- Test Configuration: test/jest-e2e.config.ts
- Docker Config: docker-compose.test.yml
- Provider Tests: src/providers/implementations/__tests__/*
- Documentation: docs/*

## Command Examples
```bash
# Run integration tests
npm run test:integration

# Run E2E tests
npm run test:e2e

# Generate documentation
npm run docs:generate

# Start test environment
docker-compose -f docker-compose.test.yml up -d
```

## Success Criteria

### 1. Test Coverage
- [ ] Unit test coverage > 90%
- [ ] Integration test coverage > 80%
- [ ] E2E test coverage > 70%
- [ ] All critical paths tested

### 2. Documentation Quality
- [ ] Complete API documentation
- [ ] Up-to-date architecture diagrams
- [ ] Clear configuration guide
- [ ] Comprehensive error handling docs

### 3. Performance Metrics
- [ ] Response time < 200ms (p95)
- [ ] Cache hit rate > 80%
- [ ] Error rate < 1%
- [ ] Resource usage within limits

## Notes
- Ensure backward compatibility
- Follow existing patterns
- Document all assumptions
- Include error handling examples
- Add monitoring hooks

// File: services\llm-service\docs\providers.md
----------------------------------------
# LLM Service Provider Documentation

## OpenRouter Integration

The LLM Service implements two different approaches for OpenRouter integration:
1. Direct API integration using axios
2. OpenAI SDK integration

### OpenRouter Direct Provider

The direct provider implementation uses axios to communicate directly with the OpenRouter API.

#### Features
- Direct HTTP requests to OpenRouter API
- Custom error handling and mapping
- Built-in retry mechanism
- Streaming support
- Request timeout handling
- Rate limit handling

#### Implementation

```typescript
class OpenRouterProvider implements LLMProvider {
  private client: AxiosInstance;
  private readonly defaultModel: string;
  private readonly maxRetries: number;
  private readonly timeout: number;

  constructor(config: OpenRouterConfig) {
    // Initialize with configuration
  }

  async complete(messages: ChatMessage[], options?: LLMRequestOptions): Promise<LLMResponse>;
  async completeStream(messages: ChatMessage[], options?: LLMRequestOptions): Promise<AsyncIterableIterator<LLMResponse>>;
  async healthCheck(): Promise<boolean>;
}
```

#### Error Handling
- Authentication errors (401)
- Rate limiting (429)
- Context length exceeded (400)
- Model not found (404)
- Timeouts (408)
- Network errors
- Unexpected errors

### OpenRouter OpenAI Provider

The OpenAI SDK provider uses the official OpenAI SDK configured for OpenRouter.

#### Features
- OpenAI SDK integration
- Automatic retries
- Type-safe requests
- Built-in error handling
- Streaming support
- Request timeout handling

#### Implementation

```typescript
class OpenRouterOpenAIProvider implements LLMProvider {
  private client: OpenAI;
  private readonly defaultModel: string;
  private readonly maxRetries: number;
  private readonly timeout: number;

  constructor(config: OpenRouterConfig) {
    // Initialize with OpenAI SDK configuration
  }

  async complete(messages: ChatMessage[], options?: LLMRequestOptions): Promise<LLMResponse>;
  async completeStream(messages: ChatMessage[], options?: LLMRequestOptions): Promise<AsyncIterableIterator<LLMResponse>>;
  async healthCheck(): Promise<boolean>;
}
```

#### Error Handling
- APIError handling
- Rate limit detection
- Context length validation
- Model availability checks
- Timeout handling
- Network error handling

## Testing Infrastructure

### Mock OpenRouter API

The test environment includes a mock OpenRouter API that simulates various scenarios:

```typescript
// Mock API endpoints
app.post('/api/v1/chat/completions', (req, res) => {
  // Handle completion requests
});

app.get('/api/v1/models', (req, res) => {
  // Handle model listing
});
```

#### Features
- Rate limiting simulation (10 requests/minute)
- Context length validation (8192 tokens)
- Configurable timeouts
- Authentication validation
- Streaming response simulation
- Error scenario simulation

### Test Cases

#### Provider Tests
```typescript
describe('OpenRouterProvider', () => {
  // Initialization tests
  it('should initialize successfully');
  it('should handle initialization failure');

  // Completion tests
  it('should complete messages successfully');
  it('should handle context length errors');
  it('should handle rate limiting');
  it('should handle timeouts');
  it('should handle invalid API key');
  it('should retry on failure');

  // Streaming tests
  it('should handle streaming responses');
  it('should handle streaming errors');
  it('should handle streaming timeouts');

  // Health check tests
  it('should return true when API is accessible');
  it('should return false when API is inaccessible');
});
```

### Integration Testing

#### Docker Compose Setup
```yaml
services:
  mock-openrouter:
    build:
      context: ./test/mock-openrouter
    ports:
      - "3001:3000"
    networks:
      - llm-test-network
```

#### Test Environment Variables
```env
OPENROUTER_API_KEY=test-key
OPENROUTER_BASE_URL=http://mock-openrouter:3000/api/v1
```

## Provider Configuration

### Direct Provider Configuration
```typescript
interface OpenRouterConfig {
  apiKey: string;
  defaultModel?: string;
  baseUrl?: string;
  siteUrl?: string;
  siteName?: string;
  maxRetries?: number;
  timeout?: number;
}
```

### OpenAI SDK Configuration
```typescript
{
  baseURL: config.baseUrl || 'https://openrouter.ai/api/v1',
  apiKey: config.apiKey,
  defaultHeaders: {
    'HTTP-Referer': config.siteUrl || '',
    'X-Title': config.siteName || '',
  },
  defaultQuery: {
    timeout: config.timeout.toString(),
  },
  maxRetries: config.maxRetries,
}
```

## Error Mapping

### Provider-Specific Errors
```typescript
enum LLMErrorType {
  PROVIDER_ERROR = 'PROVIDER_ERROR',
  RATE_LIMIT = 'RATE_LIMIT',
  CONTEXT_LENGTH = 'CONTEXT_LENGTH',
  TIMEOUT = 'TIMEOUT',
  MODEL_NOT_FOUND = 'MODEL_NOT_FOUND',
  INVALID_REQUEST = 'INVALID_REQUEST',
  UNKNOWN = 'UNKNOWN',
}
```

### Error Handling Strategy
1. Attempt to identify specific error type
2. Map to standardized LLMError
3. Include original error details
4. Apply retry strategy if applicable
5. Propagate error with context

## Performance Considerations

### Retry Strategy
- Maximum retries configurable
- Exponential backoff
- Specific error types for retries
- Timeout handling

### Rate Limiting
- Per-provider rate limits
- Global rate limiting
- Burst handling
- Queue management

### Caching
- Response caching
- Cache key generation
- TTL management
- Cache invalidation

## Monitoring

### Health Checks
- Provider availability
- API response time
- Error rates
- Rate limit status

### Metrics
- Request latency
- Token usage
- Error rates
- Cache hit rates
- Provider availability

